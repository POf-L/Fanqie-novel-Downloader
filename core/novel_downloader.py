# -*- coding: utf-8 -*-
"""
ç•ªèŒ„å°è¯´ä¸‹è½½å™¨æ ¸å¿ƒæ¨¡å— - å¯¹æ¥å®˜æ–¹API https://qkfqapi.vv9v.cn/docs
"""

# æ‰“åŒ…å…¼å®¹æ€§ä¿®å¤ - å¿…é¡»åœ¨æœ€å¼€å§‹
import sys
import os

if __package__ in (None, ""):
    _parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if _parent_dir not in sys.path:
        sys.path.insert(0, _parent_dir)

from utils.runtime_bootstrap import ensure_runtime_path, apply_packaging_fixes

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥å…¶ä»–æ¨¡å—ï¼ˆæ‰“åŒ…ç¯å¢ƒå’Œå¼€å‘ç¯å¢ƒéƒ½éœ€è¦ï¼‰
ensure_runtime_path()
apply_packaging_fixes()

import time
import requests
import re
import json
import urllib3
import threading
import signal
import inspect
from concurrent.futures import ThreadPoolExecutor, as_completed
import asyncio
from tqdm import tqdm
from typing import Optional, Dict, List, Union
from ebooklib import epub
import aiohttp
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from config.config import CONFIG, print_lock, get_headers
from utils.watermark import apply_watermark_to_chapter
from utils.async_logger import async_print, safe_print
from utils.messages import t
from core import text_utils as _tu
from core import state_store as _ss

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ===================== å®˜æ–¹APIç®¡ç†å™¨ =====================

class TokenBucket:
    """ä»¤ç‰Œæ¡¶ç®—æ³•å®ç°å¹¶å‘é€Ÿç‡é™åˆ¶ï¼Œå…è®¸çœŸæ­£çš„å¹¶å‘è¯·æ±‚"""

    def __init__(self, rate: float, capacity: int):
        """
        rate: æ¯ç§’ç”Ÿæˆçš„ä»¤ç‰Œæ•°
        capacity: æ¡¶çš„æœ€å¤§å®¹é‡
        """
        self.rate = rate
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.time()
        self._lock = asyncio.Lock()

    async def acquire(self):
        """è·å–ä¸€ä¸ªä»¤ç‰Œï¼Œå¦‚æœæ²¡æœ‰åˆ™ç­‰å¾…ï¼ˆä¼˜åŒ–ç‰ˆï¼šç§»é™¤é€’å½’è°ƒç”¨ï¼‰"""
        while True:
            async with self._lock:
                now = time.time()
                # è¡¥å……ä»¤ç‰Œ
                elapsed = now - self.last_update
                self.tokens = min(self.capacity, self.tokens + elapsed * self.rate)
                self.last_update = now

                if self.tokens >= 1:
                    self.tokens -= 1
                    return

                # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
                wait_time = (1 - self.tokens) / self.rate

            # åœ¨é”å¤–ç­‰å¾…
            await asyncio.sleep(wait_time)


class APIManager:
    """ç•ªèŒ„å°è¯´å®˜æ–¹APIç»Ÿä¸€ç®¡ç†å™¨ - https://qkfqapi.vv9v.cn/docs
    æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥ä¸¤ç§è°ƒç”¨æ–¹å¼
    """

    def __init__(self):
        self.endpoints = CONFIG["endpoints"]
        self._tls = threading.local()
        self._async_session: Optional[aiohttp.ClientSession] = None
        self.semaphore = None
        # ä½¿ç”¨ä»¤ç‰Œæ¡¶æ›¿ä»£å…¨å±€é”ï¼Œå…è®¸çœŸæ­£çš„å¹¶å‘
        self.rate_limiter: Optional[TokenBucket] = None
        # é¢„åˆå§‹åŒ–å¼‚æ­¥ä¼šè¯ä»¥å‡å°‘å¯åŠ¨å»¶è¿Ÿ
        self._session_initialized = False
        self._init_lock = asyncio.Lock()
        
        # è·å–æœ€ä¼˜èŠ‚ç‚¹ï¼ˆä¼˜å…ˆä½¿ç”¨èŠ‚ç‚¹æµ‹è¯•å™¨çš„ç»“æœï¼‰
        self.base_url = self._get_optimal_node_from_tester() or self._get_optimal_node()

    def _get_optimal_node_from_tester(self) -> Optional[str]:
        """ä»èŠ‚ç‚¹æµ‹è¯•å™¨è·å–å·²æµ‹è¯•çš„æœ€ä¼˜èŠ‚ç‚¹"""
        try:
            from utils.node_manager import get_node_tester
            tester = get_node_tester()
            if tester:
                optimal_node = tester.get_optimal_node()
                if optimal_node:
                    safe_print(f"ä½¿ç”¨èŠ‚ç‚¹æµ‹è¯•å™¨é€‰æ‹©çš„æœ€ä¼˜èŠ‚ç‚¹: {optimal_node}")
                    return optimal_node
        except Exception as e:
            safe_print(f"è·å–èŠ‚ç‚¹æµ‹è¯•å™¨ç»“æœå¤±è´¥: {e}")
        return None

    @staticmethod
    def _classify_api_sources(exclude_nodes: set = None) -> tuple:
        """ä»é…ç½®ä¸­åˆ†ç±»APIèŠ‚ç‚¹ï¼Œè¿”å› (full_download_nodes, other_nodes)"""
        full_download_nodes = []
        other_nodes = []
        exclude = exclude_nodes or set()
        for source in CONFIG.get("api_sources", []):
            if isinstance(source, dict):
                base = (source.get("base_url") or source.get("api_base_url") or "").strip().rstrip('/')
                supports_full = source.get("supports_full_download", True)
                if base and base not in exclude:
                    (full_download_nodes if supports_full else other_nodes).append(base)
            elif isinstance(source, str):
                base = str(source).strip().rstrip('/')
                if base and base not in exclude:
                    other_nodes.append(base)
        return full_download_nodes, other_nodes

    def _get_optimal_node(self) -> str:
        """è‡ªåŠ¨ä¼˜é€‰æ”¯æŒæ‰¹é‡ä¸‹è½½çš„èŠ‚ç‚¹"""
        full_nodes, other_nodes = self._classify_api_sources()
        if full_nodes:
            safe_print(f"è‡ªåŠ¨é€‰æ‹©æ”¯æŒæ‰¹é‡ä¸‹è½½çš„èŠ‚ç‚¹: {full_nodes[0]}")
            return full_nodes[0]
        if other_nodes:
            safe_print(f"é€‰æ‹©èŠ‚ç‚¹: {other_nodes[0]}")
            return other_nodes[0]
        return ""

    def _build_candidate_list(self, full_nodes: list, other_nodes: list) -> List[str]:
        """å°†åˆ†ç±»åçš„èŠ‚ç‚¹åˆ—è¡¨ç»„è£…ä¸ºå€™é€‰åˆ—è¡¨ï¼ˆå½“å‰èŠ‚ç‚¹ä¼˜å…ˆï¼‰"""
        candidates: List[str] = []
        current = (self.base_url or "").strip().rstrip('/')
        if current:
            if current in full_nodes:
                candidates.append(current)
                full_nodes.remove(current)
            elif current in other_nodes:
                candidates.append(current)
                other_nodes.remove(current)
        candidates.extend(full_nodes)
        candidates.extend(other_nodes)
        return candidates

    def _candidate_base_urls(self) -> List[str]:
        """è¿”å›å€™é€‰ API èŠ‚ç‚¹åˆ—è¡¨ï¼ˆä¼˜å…ˆæ”¯æŒæ‰¹é‡ä¸‹è½½çš„èŠ‚ç‚¹ï¼Œæ’é™¤æ•…éšœèŠ‚ç‚¹ï¼‰"""
        try:
            from utils.node_manager import get_node_tester
            node_tester = get_node_tester()
            if node_tester:
                test_results = node_tester.get_test_results()
                failed_nodes = {url for url, r in test_results.items() if not r.get('available', False)}

                try:
                    from utils.node_manager import get_health_monitor
                    health_monitor = get_health_monitor()
                    if health_monitor:
                        failed_nodes.update(health_monitor.get_failed_nodes())
                except Exception:
                    pass

                full_nodes, other_nodes = self._classify_api_sources(exclude_nodes=failed_nodes)
                if not full_nodes and not other_nodes:
                    full_nodes, other_nodes = self._classify_api_sources()
                return self._build_candidate_list(full_nodes, other_nodes)
        except Exception:
            pass

        full_nodes, other_nodes = self._classify_api_sources()
        return self._build_candidate_list(full_nodes, other_nodes)

    def _debug_log(self, message: str):
        safe_print(f"[API DEBUG] {message}")

    def _switch_base_url(self, base_url: str):
        """åˆ‡æ¢å½“å‰ç”Ÿæ•ˆèŠ‚ç‚¹"""
        normalized = (base_url or "").strip().rstrip('/')
        if not normalized:
            return
        self.base_url = normalized
        self._debug_log(f"è‡ªåŠ¨åˆ‡æ¢ API èŠ‚ç‚¹ -> {normalized}")

    def update_optimal_node(self):
        """æ›´æ–°æœ€ä¼˜èŠ‚ç‚¹ï¼ˆä»èŠ‚ç‚¹æµ‹è¯•å™¨è·å–æœ€æ–°ç»“æœï¼‰"""
        try:
            from utils.node_manager import get_node_tester
            tester = get_node_tester()
            if tester:
                optimal_node = tester.get_optimal_node()
                if optimal_node and optimal_node != self.base_url:
                    self._switch_base_url(optimal_node)
                    safe_print(f"å·²æ›´æ–°åˆ°æ–°çš„æœ€ä¼˜èŠ‚ç‚¹: {optimal_node}")
                    return True
        except Exception as e:
            safe_print(f"æ›´æ–°æœ€ä¼˜èŠ‚ç‚¹å¤±è´¥: {e}")
        return False

    def get_node_status_info(self) -> Dict:
        """è·å–å½“å‰èŠ‚ç‚¹çŠ¶æ€ä¿¡æ¯"""
        try:
            from utils.node_manager import get_node_tester
            tester = get_node_tester()
            if tester:
                return tester.get_node_status_summary()
        except Exception:
            pass
        
        # å¦‚æœèŠ‚ç‚¹æµ‹è¯•å™¨ä¸å¯ç”¨ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
        return {
            'current_node': self.base_url,
            'total_nodes': len(CONFIG.get('api_sources', [])),
            'test_completed': False
        }

    def _request_with_failover(self, endpoint: str, params: Dict) -> Optional[requests.Response]:
        """åŒæ­¥è¯·æ±‚ï¼ˆè‡ªåŠ¨æ•…éšœåˆ‡æ¢ API èŠ‚ç‚¹ï¼‰"""
        last_exception = None
        timeout = CONFIG["request_timeout"]
        candidates = self._candidate_base_urls()
        self._debug_log(
            f"è¯·æ±‚å¼€å§‹ endpoint={endpoint}, params={params}, timeout={timeout}, candidates={candidates}"
        )

        for index, base in enumerate(candidates, start=1):
            url = f"{base}{endpoint}"
            self._debug_log(f"å°è¯•èŠ‚ç‚¹[{index}/{len(candidates)}]: {url}")
            try:
                response = self._get_session().get(
                    url,
                    params=params,
                    headers=get_headers(),
                    timeout=timeout
                )

                # æˆåŠŸè¿”å›æ—¶ï¼Œè®°ä½è¯¥å¯ç”¨èŠ‚ç‚¹
                if response.status_code == 200:
                    self._debug_log(f"èŠ‚ç‚¹å“åº”æˆåŠŸ status=200: {base}")
                    if base != self.base_url:
                        safe_print(f"APIèŠ‚ç‚¹å·²è‡ªåŠ¨åˆ‡æ¢åˆ°æ›´ä¼˜èŠ‚ç‚¹: {base}")
                        self._switch_base_url(base)
                    return response

                # 5xx è§†ä¸ºèŠ‚ç‚¹æ•…éšœï¼Œå°è¯•ä¸‹ä¸€ä¸ª
                if response.status_code >= 500:
                    self._debug_log(f"èŠ‚ç‚¹å“åº”å¼‚å¸¸ status={response.status_code}ï¼Œç»§ç»­åˆ‡æ¢: {base}")
                    continue

                # 4xx ç­‰ä¸šåŠ¡é”™è¯¯ç›´æ¥è¿”å›ï¼Œé¿å…è¯¯åˆ‡æ¢
                self._debug_log(f"èŠ‚ç‚¹è¿”å›ä¸šåŠ¡çŠ¶æ€ status={response.status_code}ï¼Œåœæ­¢åˆ‡æ¢: {base}")
                return response
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                last_exception = e
                self._debug_log(f"èŠ‚ç‚¹ç½‘ç»œå¼‚å¸¸ {type(e).__name__}: {base} -> {e}")
                continue
            except requests.RequestException as e:
                last_exception = e
                self._debug_log(f"èŠ‚ç‚¹è¯·æ±‚å¼‚å¸¸ {type(e).__name__}: {base} -> {e}")
                continue

        if last_exception:
            self._debug_log(f"æ‰€æœ‰èŠ‚ç‚¹å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºæœ€åå¼‚å¸¸: {last_exception}")
            # æä¾›å‹å¥½çš„é”™è¯¯æç¤º
            error_msg = f"æ‰€æœ‰APIèŠ‚ç‚¹éƒ½ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚"
            if isinstance(last_exception, requests.exceptions.Timeout):
                error_msg = f"æ‰€æœ‰APIèŠ‚ç‚¹è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚"
            elif isinstance(last_exception, requests.exceptions.ConnectionError):
                error_msg = f"æ— æ³•è¿æ¥åˆ°ä»»ä½•APIèŠ‚ç‚¹ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚"
            safe_print(f"âš  {error_msg}")
            safe_print(f"ğŸ’¡ æç¤ºï¼šå¯ä»¥ç¨åé‡è¯•ï¼Œæˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥")
            raise last_exception
        self._debug_log("æ‰€æœ‰èŠ‚ç‚¹å°è¯•å®Œæ¯•ï¼Œæœªå¾—åˆ°æœ‰æ•ˆå“åº”")
        safe_print("âš  æ‰€æœ‰APIèŠ‚ç‚¹éƒ½æ— æ³•è¿”å›æœ‰æ•ˆå“åº”ï¼Œè¯·ç¨åé‡è¯•")
        return None

    def _get_session(self) -> requests.Session:
        """è·å–åŒæ­¥HTTPä¼šè¯"""
        sess = getattr(self._tls, 'session', None)
        if sess is None:
            sess = requests.Session()
            retries = Retry(
                total=CONFIG.get("max_retries", 3),
                backoff_factor=0.3,
                status_forcelist=(429, 500, 502, 503, 504),
                allowed_methods=("GET", "POST"),
                raise_on_status=False,
            )
            pool_size = CONFIG.get("connection_pool_size", 10)
            adapter = HTTPAdapter(
                pool_connections=pool_size, 
                pool_maxsize=pool_size, 
                max_retries=retries,
                pool_block=False
            )
            sess.mount('http://', adapter)
            sess.mount('https://', adapter)
            sess.headers.update({'Connection': 'keep-alive'})
            self._tls.session = sess
        return sess

    def _get_async_state(self) -> dict:
        """è·å–çº¿ç¨‹å±€éƒ¨çš„å¼‚æ­¥ä¼šè¯çŠ¶æ€ï¼ˆé¿å…è·¨çº¿ç¨‹/è·¨äº‹ä»¶å¾ªç¯å…±äº« aiohttp ä¼šè¯ï¼‰"""
        state = getattr(self._tls, 'async_state', None)
        if state is None:
            state = {
                'session': None,
                'initialized': False,
                'init_lock': asyncio.Lock(),
                'semaphore': None,
                'rate_limiter': None,
            }
            self._tls.async_state = state
        return state

    async def _get_async_session(self) -> aiohttp.ClientSession:
        """è·å–å¼‚æ­¥HTTPä¼šè¯ - ä¼˜åŒ–ç‰ˆï¼šå‡å°‘é‡å¤åˆå§‹åŒ–"""
        state = self._get_async_state()
        session = state.get('session')
        if state.get('initialized') and session and not session.closed:
            return session

        init_lock = state.get('init_lock')
        async with init_lock:
            # åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ï¼ˆçº¿ç¨‹å±€éƒ¨ï¼‰
            session = state.get('session')
            if state.get('initialized') and session and not session.closed:
                return session

            # ä¼˜åŒ–è¿æ¥å‚æ•°ä»¥å‡å°‘åˆå§‹åŒ–æ—¶é—´
            timeout = aiohttp.ClientTimeout(
                total=CONFIG["request_timeout"],
                connect=3,  # å‡å°‘è¿æ¥è¶…æ—¶
                sock_read=10  # å‡å°‘è¯»å–è¶…æ—¶
            )
            connector = aiohttp.TCPConnector(
                limit=CONFIG.get("connection_pool_size", 200),
                limit_per_host=min(CONFIG.get("max_workers", 30) * 3, 100),  # å¢åŠ æ¯ä¸»æœºè¿æ¥æ•°
                ttl_dns_cache=600,  # å¢åŠ DNSç¼“å­˜æ—¶é—´
                enable_cleanup_closed=True,
                force_close=False,
                keepalive_timeout=60,  # å¢åŠ keepaliveæ—¶é—´
                use_dns_cache=True  # å¯ç”¨DNSç¼“å­˜
            )
            session = aiohttp.ClientSession(
                headers=get_headers(),
                timeout=timeout,
                connector=connector,
                trust_env=True
            )
            semaphore = asyncio.Semaphore(CONFIG.get("max_workers", 30))
            # ä¼˜åŒ–ä»¤ç‰Œæ¡¶å‚æ•°ï¼šæé«˜çªå‘å¤„ç†èƒ½åŠ›
            rate = CONFIG.get("api_rate_limit", 50)
            capacity = max(CONFIG.get("max_workers", 30), rate)  # å®¹é‡è‡³å°‘ç­‰äºé€Ÿç‡
            rate_limiter = TokenBucket(rate=rate, capacity=capacity)

            state['session'] = session
            state['semaphore'] = semaphore
            state['rate_limiter'] = rate_limiter
            state['initialized'] = True

            # å…¼å®¹ï¼šä¿ç•™æ—§å­—æ®µï¼Œä½†ä¸å†ä½œä¸ºå…±äº«çŠ¶æ€ä½¿ç”¨
            self._async_session = session
            self.semaphore = semaphore
            self.rate_limiter = rate_limiter
            self._session_initialized = True

        return session

    async def close_async(self):
        """å…³é—­å¼‚æ­¥ä¼šè¯"""
        state = getattr(self._tls, 'async_state', None)
        if state:
            session = state.get('session')
            if session and not session.closed:
                await session.close()
            state['session'] = None
            state['semaphore'] = None
            state['rate_limiter'] = None
            state['initialized'] = False

        # å…¼å®¹ï¼šåŒæ­¥æ—§å­—æ®µï¼ˆä»…åæ˜ å½“å‰çº¿ç¨‹çŠ¶æ€ï¼‰
        if self._async_session and not self._async_session.closed:
            await self._async_session.close()
        self._async_session = None
        self.semaphore = None
        self.rate_limiter = None
        self._session_initialized = False

    async def pre_initialize(self):
        """é¢„åˆå§‹åŒ–å¼‚æ­¥ä¼šè¯ï¼Œå‡å°‘é¦–æ¬¡è°ƒç”¨å»¶è¿Ÿ"""
        try:
            await self._get_async_session()
            safe_print("å¼‚æ­¥ä¼šè¯é¢„åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            safe_print(f"å¼‚æ­¥ä¼šè¯é¢„åˆå§‹åŒ–å¤±è´¥: {e}")

    async def get_directory_async(self, book_id: str) -> Optional[List[Dict]]:
        """å¼‚æ­¥è·å–ç®€åŒ–ç›®å½•ï¼ˆæ›´å¿«ï¼Œæ ‡é¢˜ä¸æ•´æœ¬ä¸‹è½½å†…å®¹ä¸€è‡´ï¼‰"""
        try:
            session = await self._get_async_session()
            state = self._get_async_state()
            semaphore = state.get('semaphore')
            rate_limiter = state.get('rate_limiter')
            url = f"{self.base_url}/api/directory"
            params = {"fq_id": book_id}

            async with semaphore:
                if rate_limiter:
                    await rate_limiter.acquire()

                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data.get("code") == 200 and "data" in data:
                            lists = data["data"].get("lists", [])
                            if lists:
                                return lists
            return None
        except Exception:
            return None

    async def get_chapter_list_async(self, book_id: str) -> Optional[List[Dict]]:
        """å¼‚æ­¥è·å–ç« èŠ‚åˆ—è¡¨"""
        try:
            session = await self._get_async_session()
            state = self._get_async_state()
            semaphore = state.get('semaphore')
            rate_limiter = state.get('rate_limiter')
            url = f"{self.base_url}{self.endpoints['book']}"
            params = {"book_id": book_id}

            async with semaphore:
                if rate_limiter:
                    await rate_limiter.acquire()

                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data.get("code") == 200 and "data" in data:
                            level1_data = data["data"]
                            if isinstance(level1_data, dict) and "data" in level1_data:
                                return level1_data["data"]
                            return level1_data
            return None
        except Exception as e:
            safe_print(f"å¼‚æ­¥è·å–ç« èŠ‚åˆ—è¡¨å¤±è´¥: {str(e)}")
            return None
    
    def search_books(self, keyword: str, offset: int = 0) -> Optional[Dict]:
        """æœç´¢ä¹¦ç±"""
        try:
            params = {"key": keyword, "tab_type": "3", "offset": str(offset)}
            response = self._request_with_failover(self.endpoints['search'], params)
            if response is None:
                return None

            if response.status_code == 200:
                try:
                    data = response.json()
                    if data.get("code") == 200:
                        return data
                    else:
                        safe_print(f"æœç´¢å¤±è´¥: APIè¿”å›é”™è¯¯ç  {data.get('code')}, æ¶ˆæ¯: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                        return None
                except Exception as e:
                    # å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONï¼Œè®°å½•å“åº”å†…å®¹
                    response_text = response.text[:500]  # åªè®°å½•å‰500å­—ç¬¦
                    safe_print(f"æœç´¢å“åº”è§£æå¤±è´¥: {str(e)}")
                    safe_print(f"å“åº”å†…å®¹: {response_text}")
                    return None
            else:
                safe_print(f"æœç´¢è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return None
        except Exception as e:
            safe_print(t("dl_search_error", str(e)))
            return None
    
    def get_book_detail(self, book_id: str) -> Optional[Dict]:
        """è·å–ä¹¦ç±è¯¦æƒ…ï¼Œè¿”å› dict æˆ– Noneï¼Œå¦‚æœä¹¦ç±ä¸‹æ¶ä¼šè¿”å› {'_error': 'BOOK_REMOVE'}"""
        try:
            params = {"book_id": book_id}
            response = self._request_with_failover(self.endpoints['detail'], params)
            if response is None:
                return None

            if response.status_code == 200:
                try:
                    data = response.json()
                    if data.get("code") == 200 and "data" in data:
                        level1_data = data["data"]
                        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚ä¹¦ç±ä¸‹æ¶ï¼‰
                        if isinstance(level1_data, dict):
                            inner_msg = level1_data.get("message", "")
                            inner_code = level1_data.get("code")
                            if inner_msg == "BOOK_REMOVE" or inner_code == 101109:
                                return {"_error": "BOOK_REMOVE", "_message": "ä¹¦ç±å·²ä¸‹æ¶"}
                            if "data" in level1_data:
                                inner_data = level1_data["data"]
                                # å¦‚æœå†…å±‚ data æ˜¯ç©ºçš„ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸‹æ¶
                                if isinstance(inner_data, dict) and not inner_data and inner_msg:
                                    return {"_error": inner_msg, "_message": inner_msg}
                                return inner_data
                        return level1_data
                except Exception as e:
                    safe_print(f"ä¹¦ç±è¯¦æƒ…å“åº”è§£æå¤±è´¥: {str(e)}")
                    return None
            return None
        except Exception as e:
            safe_print(t("dl_detail_error", str(e)))
            return None
    
    def get_directory(self, book_id: str) -> Optional[List[Dict]]:
        """è·å–ç®€åŒ–ç›®å½•ï¼ˆæ›´å¿«ï¼Œæ ‡é¢˜ä¸æ•´æœ¬ä¸‹è½½å†…å®¹ä¸€è‡´ï¼‰
        GET /api/directory - å‚æ•°: fq_id
        """
        try:
            params = {"fq_id": book_id}
            response = self._request_with_failover('/api/directory', params)
            if response is None:
                return None
            
            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    lists = data["data"].get("lists", [])
                    if lists:
                        return lists
            return None
        except Exception:
            return None
    
    def get_chapter_list(self, book_id: str) -> Optional[List[Dict]]:
        """è·å–ç« èŠ‚åˆ—è¡¨"""
        try:
            safe_print(t("dl_chapter_list_start", book_id))
                
            params = {"book_id": book_id}
            response = self._request_with_failover(self.endpoints['book'], params)
            if response is None:
                return None
            
            safe_print(t("dl_chapter_list_resp", response.status_code))
            
            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    level1_data = data["data"]
                    if isinstance(level1_data, dict) and "data" in level1_data:
                        return level1_data["data"]
                    return level1_data
            return None
        except Exception as e:
            safe_print(t("dl_chapter_list_error", str(e)))
            return None
    
    def get_chapter_content(self, item_id: str) -> Optional[Dict]:
        """è·å–ç« èŠ‚å†…å®¹(åŒæ­¥)
        ä¼˜å…ˆä½¿ç”¨ /api/chapter ç®€åŒ–æ¥å£ï¼Œå¤±è´¥æ—¶å›é€€åˆ° /api/content
        """
        try:
            # ä¼˜å…ˆå°è¯•ç®€åŒ–çš„ /api/chapter æ¥å£ï¼ˆæ›´ç¨³å®šï¼‰
            chapter_endpoint = self.endpoints.get('chapter', '/api/chapter')
            params = {"item_id": item_id}
            response = self._request_with_failover(chapter_endpoint, params)
            if response is None:
                return None
            
            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    return data["data"]
            
            # å›é€€åˆ° /api/content æ¥å£
            params = {"tab": "å°è¯´", "item_id": item_id}
            response = self._request_with_failover(self.endpoints['content'], params)
            if response is None:
                return None
            
            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    return data["data"]
            return None
        except Exception as e:
            safe_print(t("dl_content_error", str(e)))
            return None


    async def get_chapter_content_async(self, item_id: str) -> Optional[Dict]:
        """è·å–ç« èŠ‚å†…å®¹(å¼‚æ­¥)
        ä¼˜å…ˆä½¿ç”¨ /api/chapter ç®€åŒ–æ¥å£ï¼Œå¤±è´¥æ—¶å›é€€åˆ° /api/content
        ä½¿ç”¨ä»¤ç‰Œæ¡¶ç®—æ³•å®ç°çœŸæ­£çš„å¹¶å‘é€Ÿç‡é™åˆ¶
        """
        max_retries = CONFIG.get("max_retries", 3)
        session = await self._get_async_session()
        state = self._get_async_state()
        semaphore = state.get('semaphore')
        rate_limiter = state.get('rate_limiter')

        # ä½¿ç”¨ä»¤ç‰Œæ¡¶è¿›è¡Œé€Ÿç‡é™åˆ¶ï¼Œå…è®¸çœŸæ­£çš„å¹¶å‘
        async with semaphore:
            if rate_limiter:
                await rate_limiter.acquire()

            # ä¼˜å…ˆå°è¯•ç®€åŒ–çš„ /api/chapter æ¥å£
            chapter_endpoint = self.endpoints.get('chapter', '/api/chapter')
            url = f"{self.base_url}{chapter_endpoint}"
            params = {"item_id": item_id}

            for attempt in range(max_retries):
                try:
                    async with session.get(url, params=params) as response:
                        if response.status == 200:
                            data = await response.json()
                            if data.get("code") == 200 and "data" in data:
                                return data["data"]
                        elif response.status == 429:
                            await asyncio.sleep(min(2 ** attempt, 10))
                            continue
                        break  # å…¶ä»–é”™è¯¯ï¼Œå°è¯•å¤‡ç”¨æ¥å£
                except asyncio.TimeoutError:
                    if attempt < max_retries - 1:
                        await asyncio.sleep(CONFIG.get("retry_delay", 2) * (attempt + 1))
                        continue
                    break
                except Exception:
                    if attempt < max_retries - 1:
                        await asyncio.sleep(0.3)
                        continue
                    break

            # å›é€€åˆ° /api/content æ¥å£
            url = f"{self.base_url}{self.endpoints['content']}"
            params = {"tab": "å°è¯´", "item_id": item_id}

            for attempt in range(max_retries):
                try:
                    async with session.get(url, params=params) as response:
                        if response.status == 200:
                            data = await response.json()
                            if data.get("code") == 200 and "data" in data:
                                return data["data"]
                        elif response.status == 429:
                            await asyncio.sleep(min(2 ** attempt, 10))
                            continue
                        return None
                except asyncio.TimeoutError:
                    if attempt < max_retries - 1:
                        await asyncio.sleep(CONFIG.get("retry_delay", 2) * (attempt + 1))
                        continue
                    return None
                except Exception:
                    if attempt < max_retries - 1:
                        await asyncio.sleep(0.3)
                        continue
                    return None

            return None

    # ===================== æ–°å¢APIæ–¹æ³• =====================

    def get_audiobook_content(self, item_id: str, tone_id: str = "0") -> Optional[Dict]:
        """è·å–å¬ä¹¦éŸ³é¢‘å†…å®¹

        Args:
            item_id: ç« èŠ‚ID
            tone_id: éŸ³è‰²IDï¼Œé»˜è®¤ä¸º"0"

        Returns:
            åŒ…å«éŸ³é¢‘URLçš„å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            url = f"{self.base_url}{self.endpoints['content']}"
            params = {"tab": "å¬ä¹¦", "item_id": item_id, "tone_id": tone_id}
            response = self._get_session().get(url, params=params, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    return data["data"]
            return None
        except Exception as e:
            with print_lock:
                print(f"è·å–å¬ä¹¦å†…å®¹å¤±è´¥: {e}")
            return None

    def get_drama_content(self, item_id: str) -> Optional[Dict]:
        """è·å–çŸ­å‰§è§†é¢‘å†…å®¹

        Args:
            item_id: è§†é¢‘/ç« èŠ‚ID

        Returns:
            åŒ…å«è§†é¢‘ä¿¡æ¯çš„å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            url = f"{self.base_url}{self.endpoints['content']}"
            params = {"tab": "çŸ­å‰§", "item_id": item_id}
            response = self._get_session().get(url, params=params, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    return data["data"]
            return None
        except Exception as e:
            with print_lock:
                print(f"è·å–çŸ­å‰§å†…å®¹å¤±è´¥: {e}")
            return None

    def get_manga_content(self, item_id: str, show_html: str = "0", async_mode: str = "1") -> Optional[Dict]:
        """è·å–æ¼«ç”»å›¾ç‰‡å†…å®¹

        Args:
            item_id: æ¼«ç”»ç« èŠ‚ID
            show_html: æ˜¯å¦è¿”å›HTMLæ ¼å¼ ("0" æˆ– "1")
            async_mode: æ˜¯å¦å¼‚æ­¥æ¨¡å¼ ("0" æˆ– "1")

        Returns:
            åŒæ­¥æ¨¡å¼è¿”å›å›¾ç‰‡æ•°æ®ï¼Œå¼‚æ­¥æ¨¡å¼è¿”å›ä»»åŠ¡ID
        """
        try:
            url = f"{self.base_url}{self.endpoints['content']}"
            params = {"tab": "æ¼«ç”»", "item_id": item_id, "show_html": show_html, "async": async_mode}
            response = self._get_session().get(url, params=params, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    return data["data"]
            return None
        except Exception as e:
            with print_lock:
                print(f"è·å–æ¼«ç”»å†…å®¹å¤±è´¥: {e}")
            return None

    def get_manga_progress(self, task_id: str) -> Optional[Dict]:
        """æŸ¥è¯¢æ¼«ç”»ä¸‹è½½è¿›åº¦

        Args:
            task_id: å¼‚æ­¥ä»»åŠ¡ID

        Returns:
            åŒ…å«è¿›åº¦ä¿¡æ¯çš„å­—å…¸
        """
        try:
            endpoint = self.endpoints.get('manga_progress', '/api/manga/progress')
            url = f"{self.base_url}{endpoint}/{task_id}"
            response = self._get_session().get(url, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    return data["data"]
            return None
        except Exception as e:
            with print_lock:
                print(f"æŸ¥è¯¢æ¼«ç”»è¿›åº¦å¤±è´¥: {e}")
            return None

    def get_ios_content(self, item_id: str) -> Optional[Dict]:
        """é€šè¿‡iOSæ¥å£è·å–ç« èŠ‚å†…å®¹ï¼ˆä½¿ç”¨8402ç®—æ³•ç­¾åï¼‰

        Args:
            item_id: ç« èŠ‚ID

        Returns:
            ç« èŠ‚å†…å®¹å­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            endpoint = self.endpoints.get('ios_content', '/api/ios/content')
            url = f"{self.base_url}{endpoint}"
            params = {"item_id": item_id}
            response = self._get_session().get(url, params=params, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    return data["data"]
            return None
        except Exception as e:
            with print_lock:
                print(f"è·å–iOSå†…å®¹å¤±è´¥: {e}")
            return None

    def register_ios_device(self) -> Optional[Dict]:
        """æ³¨å†Œæ–°çš„iOSè®¾å¤‡åˆ°è®¾å¤‡æ± 

        Returns:
            æ³¨å†Œç»“æœ
        """
        try:
            endpoint = self.endpoints.get('ios_register', '/api/ios/register')
            url = f"{self.base_url}{endpoint}"
            response = self._get_session().get(url, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200:
                    return data.get("data", data)
            return None
        except Exception as e:
            with print_lock:
                print(f"æ³¨å†ŒiOSè®¾å¤‡å¤±è´¥: {e}")
            return None

    def get_device_pool(self) -> Optional[Dict]:
        """è·å–è®¾å¤‡æ± æ•´ä½“çŠ¶æ€

        Returns:
            æ‰€æœ‰è®¾å¤‡çŠ¶æ€ä¿¡æ¯
        """
        try:
            endpoint = self.endpoints.get('device_pool', '/api/device/pool')
            url = f"{self.base_url}{endpoint}"
            response = self._get_session().get(url, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200:
                    return data.get("data", data)
            return None
        except Exception as e:
            with print_lock:
                print(f"è·å–è®¾å¤‡æ± çŠ¶æ€å¤±è´¥: {e}")
            return None

    def register_device(self, platform: str = "android") -> Optional[Dict]:
        """æ³¨å†Œæ–°è®¾å¤‡åˆ°è®¾å¤‡æ± 

        Args:
            platform: å¹³å°ç±»å‹ ("android" æˆ– "ios")

        Returns:
            æ³¨å†Œç»“æœ
        """
        try:
            endpoint = self.endpoints.get('device_register', '/api/device/register')
            url = f"{self.base_url}{endpoint}"
            params = {"platform": platform}
            response = self._get_session().get(url, params=params, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200:
                    return data.get("data", data)
            return None
        except Exception as e:
            with print_lock:
                print(f"æ³¨å†Œè®¾å¤‡å¤±è´¥: {e}")
            return None

    def get_device_status(self, platform: str = "android") -> Optional[Dict]:
        """è·å–æŒ‡å®šå¹³å°çš„è®¾å¤‡çŠ¶æ€

        Args:
            platform: å¹³å°ç±»å‹ ("android" æˆ– "ios")

        Returns:
            è®¾å¤‡çŠ¶æ€ä¿¡æ¯
        """
        try:
            endpoint = self.endpoints.get('device_status', '/api/device/status')
            url = f"{self.base_url}{endpoint}"
            params = {"platform": platform}
            response = self._get_session().get(url, params=params, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200:
                    return data.get("data", data)
            return None
        except Exception as e:
            with print_lock:
                print(f"è·å–è®¾å¤‡çŠ¶æ€å¤±è´¥: {e}")
            return None

    def get_raw_content(self, item_id: str) -> Optional[Dict]:
        """è·å–æœªå¤„ç†çš„åŸå§‹ç« èŠ‚å†…å®¹

        Args:
            item_id: ç« èŠ‚ID

        Returns:
            å®Œæ•´çš„åŸå§‹å“åº”æ•°æ®
        """
        try:
            endpoint = self.endpoints.get('raw_full', '/api/raw_full')
            url = f"{self.base_url}{endpoint}"
            params = {"item_id": item_id}
            response = self._get_session().get(url, params=params, headers=get_headers(), timeout=CONFIG["request_timeout"])

            if response.status_code == 200:
                data = response.json()
                if data.get("code") == 200 and "data" in data:
                    return data["data"]
            return None
        except Exception as e:
            with print_lock:
                print(f"è·å–åŸå§‹å†…å®¹å¤±è´¥: {e}")
            return None

    # ===================== æ–°å¢APIæ–¹æ³•ç»“æŸ =====================

    def get_full_content(self, book_id: str) -> Optional[Union[str, Dict[str, str]]]:
        """è·å–æ•´æœ¬å°è¯´å†…å®¹ï¼Œæ”¯æŒå¤šèŠ‚ç‚¹è‡ªåŠ¨åˆ‡æ¢

        è¿”å›ï¼š
        - dict: æ‰¹é‡æ¨¡å¼è¿”å›çš„ {item_id: content}ï¼ˆæœ€å¯é ï¼Œå¯ä¸ç›®å½•æŒ‰ item_id ç²¾å‡†å¯¹é½ï¼‰
        - str: æ–‡æœ¬æ¨¡å¼è¿”å›çš„æ•´æœ¬å†…å®¹ï¼ˆå…¼å®¹æ—§æ¥å£/èŠ‚ç‚¹ï¼‰
        """
        max_retries = max(1, int(CONFIG.get("max_retries", 3) or 3))
        api_sources = CONFIG.get("api_sources", [])

        def _extract_bulk_map(payload) -> Optional[Dict[str, str]]:
            if not isinstance(payload, dict):
                return None
            nested = payload.get('data')
            if not isinstance(nested, dict):
                return None

            keys = list(nested.keys())
            if not keys:
                return None

            sample = keys[:min(5, len(keys))]
            if not all(str(k).isdigit() for k in sample):
                return None

            result: Dict[str, str] = {}
            for k, v in nested.items():
                item_id = str(k)
                content = None
                if isinstance(v, str):
                    content = v
                elif isinstance(v, dict):
                    content = (
                        v.get("content")
                        or v.get("text")
                        or v.get("raw")
                        or v.get("raw_text")
                        or ""
                    )
                if isinstance(content, str) and content.strip():
                    result[item_id] = content

            return result or None

        def _extract_text(payload) -> Optional[str]:
            if isinstance(payload, str):
                return payload
            if isinstance(payload, dict):
                nested = payload.get('data')
                if isinstance(nested, str):
                    return nested
                if isinstance(nested, dict):
                    for key in ("content", "text", "raw", "raw_text", "full_text"):
                        val = nested.get(key)
                        if isinstance(val, str):
                            return val
                for key in ("content", "text", "raw", "raw_text", "full_text"):
                    val = payload.get(key)
                    if isinstance(val, str):
                        return val
            return None

        endpoint = self.endpoints.get('content')
        if not endpoint:
            return None

        # å°è¯•å¯¼å…¥èŠ‚ç‚¹ç¼“å­˜ï¼ˆweb_appæ¨¡å—å¯èƒ½æœªåŠ è½½ï¼‰
        try:
            from web.web_app import PROBED_NODES_CACHE
        except ImportError:
            PROBED_NODES_CACHE = {}

        def _is_node_available(url: str) -> bool:
            """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å¯ç”¨ï¼ˆå¯åŠ¨æ—¶æ¢æµ‹é€šè¿‡ï¼‰"""
            url = (url or "").strip().rstrip('/')
            if not PROBED_NODES_CACHE:
                return True  # ç¼“å­˜ä¸ºç©ºæ—¶é»˜è®¤å¯ç”¨
            if url not in PROBED_NODES_CACHE:
                return True  # æœªæ¢æµ‹çš„èŠ‚ç‚¹é»˜è®¤å¯ç”¨
            return PROBED_NODES_CACHE[url].get('available', False)

        def _supports_full_download(url: str) -> bool:
            """æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦æ”¯æŒæ•´æœ¬ä¸‹è½½"""
            url = (url or "").strip().rstrip('/')
            if not PROBED_NODES_CACHE:
                return True  # ç¼“å­˜ä¸ºç©ºæ—¶é»˜è®¤æ”¯æŒ
            if url not in PROBED_NODES_CACHE:
                return True  # æœªæ¢æµ‹çš„èŠ‚ç‚¹é»˜è®¤æ”¯æŒ
            return PROBED_NODES_CACHE[url].get('supports_full_download', True)

        # æ„å»ºè¦å°è¯•çš„èŠ‚ç‚¹åˆ—è¡¨ï¼ˆä¼˜å…ˆå½“å‰ base_urlï¼Œè·³è¿‡ä¸å¯ç”¨å’Œä¸æ”¯æŒæ•´æœ¬ä¸‹è½½çš„èŠ‚ç‚¹ï¼‰
        urls_to_try: List[str] = []
        if self.base_url and _is_node_available(self.base_url) and _supports_full_download(self.base_url):
            urls_to_try.append(self.base_url)
        for source in api_sources:
            base = ""
            supports_full = True
            if isinstance(source, dict):
                base = source.get("base_url", "") or source.get("api_base_url", "")
                supports_full = source.get("supports_full_download", True)
            elif isinstance(source, str):
                base = source
            base = (base or "").strip().rstrip('/')
            if base and base not in urls_to_try:
                # è·³è¿‡ä¸æ”¯æŒæ•´æœ¬ä¸‹è½½çš„èŠ‚ç‚¹
                if not supports_full:
                    with print_lock:
                        print(f"[DEBUG] è·³è¿‡ä¸æ”¯æŒæ•´æœ¬ä¸‹è½½çš„èŠ‚ç‚¹: {base}")
                    continue
                # è·³è¿‡å¯åŠ¨æ—¶æ¢æµ‹å¤±è´¥çš„èŠ‚ç‚¹
                if not _is_node_available(base):
                    with print_lock:
                        print(f"[DEBUG] è·³è¿‡ä¸å¯ç”¨èŠ‚ç‚¹: {base}")
                    continue
                urls_to_try.append(base)

        if not urls_to_try:
            with print_lock:
                print("[DEBUG] æ²¡æœ‰å¯ç”¨çš„æ”¯æŒæ•´æœ¬ä¸‹è½½çš„èŠ‚ç‚¹")
            return None

        # ä¸‹è½½æ¨¡å¼ï¼šæ‰¹é‡æ¨¡å¼ä¼˜å…ˆï¼ˆå¯æŒ‰ item_id å¯¹é½ï¼‰
        download_modes = [
            {"tab": "æ‰¹é‡", "book_id": book_id},
            {"tab": "ä¸‹è½½", "book_id": book_id},
        ]

        headers = get_headers()
        headers['Connection'] = 'close'

        session = self._get_session()
        connect_timeout = 10
        read_timeout = max(120, int((CONFIG.get("request_timeout", 30) or 30) * 10))
        timeout = (connect_timeout, read_timeout)

        transient_errors = (
            requests.exceptions.ConnectionError,
            requests.exceptions.Timeout,
            requests.exceptions.ChunkedEncodingError,
            requests.exceptions.ContentDecodingError,
        )

        for base_url in urls_to_try:
            url = f"{base_url}{endpoint}"

            for mode in download_modes:
                for attempt in range(max_retries):
                    try:
                        with print_lock:
                            print(
                                f"[DEBUG] å°è¯•èŠ‚ç‚¹ {base_url}, æ¨¡å¼ tab={mode.get('tab')} "
                                f"({attempt + 1}/{max_retries})"
                            )

                        with session.get(
                            url,
                            params=mode,
                            headers=headers,
                            timeout=timeout,
                            stream=True,
                        ) as response:
                            status_code = response.status_code
                            resp_headers = dict(response.headers)
                            resp_encoding = response.encoding

                            if status_code == 400:
                                # è¯¥èŠ‚ç‚¹ä¸æ”¯æŒæ­¤æ¨¡å¼ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ¨¡å¼
                                break
                            if status_code != 200:
                                # 429/5xx äº¤ç»™ä¼šè¯é‡è¯•ï¼›è¿™é‡Œé¢å¤–åšå°‘é‡é€€é¿
                                if status_code in (429, 500, 502, 503, 504) and attempt < max_retries - 1:
                                    time.sleep(min(2 ** attempt, 10))
                                    continue
                                break

                            raw_buf = bytearray()
                            for chunk in response.iter_content(chunk_size=131072):
                                if chunk:
                                    raw_buf.extend(chunk)
                            raw_content = bytes(raw_buf)

                        if len(raw_content) < 1000:
                            break

                        content_type = (resp_headers.get('content-type') or '').lower()
                        is_json_like = 'application/json' in content_type or raw_content[:1] in (b'{', b'[')

                        if is_json_like:
                            try:
                                data = json.loads(raw_content.decode('utf-8', errors='ignore'))
                            except Exception:
                                data = None

                            if not data:
                                if attempt < max_retries - 1:
                                    time.sleep(min(2 ** attempt, 10))
                                    continue
                                break

                            bulk_map = _extract_bulk_map(data)
                            if bulk_map:
                                with print_lock:
                                    print(f"[DEBUG] æ€¥é€Ÿä¸‹è½½æˆåŠŸï¼ŒèŠ‚ç‚¹: {base_url}, æ¨¡å¼: tab={mode.get('tab')}")
                                return bulk_map

                            text_from_json = _extract_text(data)
                            if text_from_json and len(text_from_json) > 1000:
                                with print_lock:
                                    print(f"[DEBUG] æ€¥é€Ÿä¸‹è½½æˆåŠŸï¼ŒèŠ‚ç‚¹: {base_url}, æ¨¡å¼: tab={mode.get('tab')}")
                                return text_from_json

                            break

                        encoding = resp_encoding or 'utf-8'
                        text = raw_content.decode(encoding, errors='replace')
                        if len(text) > 1000:
                            with print_lock:
                                print(f"[DEBUG] æ€¥é€Ÿä¸‹è½½æˆåŠŸï¼ŒèŠ‚ç‚¹: {base_url}, æ¨¡å¼: tab={mode.get('tab')}")
                            return text

                        break

                    except transient_errors as e:
                        if attempt < max_retries - 1:
                            time.sleep(min(2 ** attempt, 10))
                            continue
                        with print_lock:
                            print(
                                f"[DEBUG] èŠ‚ç‚¹ {base_url} ä¸‹è½½å¤±è´¥: {type(e).__name__}ï¼Œ"
                                f"åˆ‡æ¢æ¨¡å¼/èŠ‚ç‚¹"
                            )
                    except Exception as e:
                        with print_lock:
                            print(f"[DEBUG] èŠ‚ç‚¹ {base_url} å¼‚å¸¸: {type(e).__name__}")
                        break

        with print_lock:
            print(t("dl_full_content_error", "æ‰€æœ‰èŠ‚ç‚¹å‡å¤±è´¥"))
        return None

def _normalize_title(title: str) -> str:
    """æ ‡å‡†åŒ–ç« èŠ‚æ ‡é¢˜ï¼Œç”¨äºæ¨¡ç³ŠåŒ¹é…"""
    return _tu.normalize_title(title)


def _extract_title_core(title: str) -> str:
    """æå–æ ‡é¢˜æ ¸å¿ƒéƒ¨åˆ†ï¼ˆå»æ‰ç« èŠ‚å·å‰ç¼€ï¼‰"""
    return _tu.extract_title_core(title)


def parse_novel_text_with_catalog(text: str, catalog: List[Dict]) -> List[Dict]:
    """ä½¿ç”¨ç›®å½•æ¥å£çš„ç« èŠ‚æ ‡é¢˜æ¥åˆ†å‰²æ•´æœ¬å°è¯´å†…å®¹
    
    Args:
        text: æ•´æœ¬å°è¯´çš„çº¯æ–‡æœ¬å†…å®¹
        catalog: ç›®å½•æ¥å£è¿”å›çš„ç« èŠ‚åˆ—è¡¨ [{'title': '...', 'id': '...', 'index': ...}, ...]
    
    Returns:
        å¸¦å†…å®¹çš„ç« èŠ‚åˆ—è¡¨ [{'title': '...', 'id': '...', 'index': ..., 'content': '...'}, ...]
    """
    return _tu.parse_novel_text_with_catalog(text, catalog)


def parse_novel_text(text: str) -> List[Dict]:
    """è§£ææ•´æœ¬å°è¯´æ–‡æœ¬ï¼Œåˆ†ç¦»ç« èŠ‚ï¼ˆæ— ç›®å½•æ—¶çš„é™çº§æ–¹æ¡ˆï¼‰"""
    return _tu.parse_novel_text(text)


class APIManagerExt(APIManager):
    """æ‰©å±•çš„APIç®¡ç†å™¨ï¼Œæ·»åŠ å¼‚æ­¥æ‰¹é‡ä¸‹è½½åŠŸèƒ½"""
    
    async def download_chapters_async(self, chapters: List[Dict], progress_callback=None) -> Dict[int, Dict]:
        """å¼‚æ­¥æ‰¹é‡ä¸‹è½½ç« èŠ‚ - æ›¿ä»£ThreadPoolExecutorçš„é«˜æ€§èƒ½å®ç°"""
        results = {}
        semaphore = asyncio.Semaphore(CONFIG.get("max_workers", 30))
        
        async def download_single(chapter):
            async with semaphore:
                content = await self.get_chapter_content_async(chapter["id"])
                if content and content.get('content'):
                    return chapter['index'], {
                        'title': chapter['title'],
                        'content': content.get('content', '')
                    }
                return None
        
        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨
        tasks = [download_single(ch) for ch in chapters]
        
        # æ‰¹é‡æ‰§è¡Œ
        completed = 0
        for coro in asyncio.as_completed(tasks):
            result = await coro
            if result:
                idx, data = result
                results[idx] = data
                completed += 1
                if progress_callback:
                    progress = int((completed / len(chapters)) * 100)
                    progress_callback(progress, f"å·²ä¸‹è½½ {completed}/{len(chapters)} ç« ")
        
        return results

    def download_chapters(self, chapters: List[Dict], progress_callback=None) -> Dict[int, Dict]:
        """ä¸‹è½½ç« èŠ‚åˆ—è¡¨ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦åœ¨å¼‚æ­¥ç¯å¢ƒä¸­
        try:
            loop = asyncio.get_running_loop()
            # åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œåˆ›å»ºä»»åŠ¡
            return loop.create_task(self.download_chapters_async(chapters, progress_callback))
        except RuntimeError:
            # ä¸åœ¨å¼‚æ­¥ç¯å¢ƒä¸­ï¼Œè¿è¡Œæ–°çš„äº‹ä»¶å¾ªç¯
            return asyncio.run(self.download_chapters_async(chapters, progress_callback))


# å…¨å±€APIç®¡ç†å™¨å®ä¾‹
api_manager = None

def get_api_manager():
    """è·å–APIç®¡ç†å™¨å®ä¾‹"""
    global api_manager
    if api_manager is None:
        api_manager = APIManagerExt()
    return api_manager


# ===================== è¾…åŠ©å‡½æ•° =====================

def sanitize_filename(name: str) -> str:
    r"""
    æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
    
    Args:
        name: åŸå§‹æ–‡ä»¶å
    
    Returns:
        æ¸…ç†åçš„æ–‡ä»¶åï¼Œéæ³•å­—ç¬¦ (\ / : * ? " < > |) æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    """
    return _tu.sanitize_filename(name)


def generate_filename(book_name: str, author_name: str, extension: str) -> str:
    """
    ç”Ÿæˆæ–‡ä»¶å
    
    Args:
        book_name: ä¹¦å
        author_name: ä½œè€…å (å¯ä¸ºç©º)
        extension: æ–‡ä»¶æ‰©å±•å (txt/epub)
    
    Returns:
        æ ¼å¼åŒ–çš„æ–‡ä»¶å: "{ä¹¦å} ä½œè€…ï¼š{ä½œè€…å}.{æ‰©å±•å}" æˆ– "{ä¹¦å}.{æ‰©å±•å}"
    """
    return _tu.generate_filename(book_name, author_name, extension)


def process_chapter_content(content):
    """å¤„ç†ç« èŠ‚å†…å®¹"""
    return _tu.process_chapter_content(content, watermark_func=apply_watermark_to_chapter)


def _get_status_file_path(book_id: str) -> str:
    """è·å–ä¸‹è½½çŠ¶æ€æ–‡ä»¶è·¯å¾„ï¼ˆä¿å­˜åœ¨ä¸´æ—¶ç›®å½•ï¼Œä¸æ±¡æŸ“å°è¯´ç›®å½•ï¼‰"""
    return _ss.get_status_file_path(book_id)


def _get_content_file_path(book_id: str) -> str:
    """è·å–å·²ä¸‹è½½å†…å®¹æ–‡ä»¶è·¯å¾„"""
    return _ss.get_content_file_path(book_id)


def _get_status_dir() -> str:
    """è·å–ä¸‹è½½çŠ¶æ€ç›®å½•ï¼ˆä¸´æ—¶ç›®å½•ï¼‰ã€‚"""
    return _ss.get_status_dir()


def load_status(book_id: str):
    """åŠ è½½ä¸‹è½½çŠ¶æ€ï¼ˆä»ä¸´æ—¶ç›®å½•è¯»å–ï¼‰"""
    return _ss.load_status(book_id)


def load_saved_content(book_id: str) -> dict:
    """åŠ è½½å·²ä¿å­˜çš„ç« èŠ‚å†…å®¹
    
    Args:
        book_id: ä¹¦ç±ID
    
    Returns:
        dict: å·²ä¿å­˜çš„ç« èŠ‚å†…å®¹ {index: {'title': ..., 'content': ...}}
    """
    return _ss.load_saved_content(book_id)


def save_status(book_id: str, downloaded_ids):
    """ä¿å­˜ä¸‹è½½çŠ¶æ€ï¼ˆä¿å­˜åˆ°ä¸´æ—¶ç›®å½•ï¼‰"""
    try:
        _ss.save_status(book_id, downloaded_ids)
    except Exception as e:
        with print_lock:
            print(t("dl_save_status_fail", str(e)))


def save_content(book_id: str, chapter_results: dict):
    """ä¿å­˜å·²ä¸‹è½½çš„ç« èŠ‚å†…å®¹
    
    Args:
        book_id: ä¹¦ç±ID
        chapter_results: ç« èŠ‚å†…å®¹ {index: {'title': ..., 'content': ...}}
    """
    try:
        _ss.save_content(book_id, chapter_results)
    except Exception as e:
        with print_lock:
            print(f"ä¿å­˜ç« èŠ‚å†…å®¹å¤±è´¥: {str(e)}")


def clear_status(book_id: str):
    """æ¸…é™¤ä¸‹è½½çŠ¶æ€ï¼ˆä¸‹è½½å®Œæˆåè°ƒç”¨ï¼‰"""
    try:
        _ss.clear_status(book_id)
    except Exception:
        pass


def has_saved_state(book_id: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„ä¸‹è½½çŠ¶æ€
    
    Args:
        book_id: ä¹¦ç±ID
    
    Returns:
        bool: æ˜¯å¦æœ‰å·²ä¿å­˜çš„çŠ¶æ€
    """
    return _ss.has_saved_state(book_id)


def analyze_download_completeness(chapter_results: dict, expected_chapters: list = None, log_func=None) -> dict:
    """
    åˆ†æä¸‹è½½å®Œæ•´æ€§
    
    Args:
        chapter_results: å·²ä¸‹è½½çš„ç« èŠ‚ç»“æœ {index: {'title': ..., 'content': ...}}
        expected_chapters: æœŸæœ›çš„ç« èŠ‚åˆ—è¡¨ [{'id': ..., 'title': ..., 'index': ...}]
        log_func: æ—¥å¿—è¾“å‡ºå‡½æ•°
    
    Returns:
        åˆ†æç»“æœå­—å…¸:
        - total_expected: æœŸæœ›æ€»ç« èŠ‚æ•°
        - total_downloaded: å·²ä¸‹è½½ç« èŠ‚æ•°
        - missing_indices: ç¼ºå¤±çš„ç« èŠ‚ç´¢å¼•åˆ—è¡¨
        - order_correct: é¡ºåºæ˜¯å¦æ­£ç¡®
        - completeness_percent: å®Œæ•´åº¦ç™¾åˆ†æ¯”
    """
    def log(msg, progress=-1):
        if log_func:
            log_func(msg, progress)
        else:
            print(msg)
    
    result = {
        'total_expected': 0,
        'total_downloaded': len(chapter_results),
        'missing_indices': [],
        'order_correct': True,
        'completeness_percent': 100.0
    }
    
    if not chapter_results:
        log(t("dl_analyze_no_chapters"))
        result['completeness_percent'] = 0
        return result
    
    # è·å–å·²ä¸‹è½½çš„ç« èŠ‚ç´¢å¼•
    downloaded_indices = set(chapter_results.keys())
    
    # å¦‚æœæœ‰æœŸæœ›çš„ç« èŠ‚åˆ—è¡¨ï¼Œè¿›è¡Œå®Œæ•´æ€§æ¯”å¯¹
    if expected_chapters:
        expected_indices = set(ch['index'] for ch in expected_chapters)
        result['total_expected'] = len(expected_indices)
        
        # æŸ¥æ‰¾ç¼ºå¤±çš„ç« èŠ‚
        missing_indices = expected_indices - downloaded_indices
        result['missing_indices'] = sorted(list(missing_indices))
        
        if missing_indices:
            missing_count = len(missing_indices)
            log(t("dl_analyze_summary", len(expected_indices), len(downloaded_indices), missing_count))
            
            # æ˜¾ç¤ºéƒ¨åˆ†ç¼ºå¤±ç« èŠ‚ä¿¡æ¯
            if missing_count <= 10:
                missing_titles = []
                for ch in expected_chapters:
                    if ch['index'] in missing_indices:
                        missing_titles.append(f"{t('dl_chapter_title', ch['index']+1)}: {ch['title']}")
                log(t("dl_analyze_missing", ', '.join(missing_titles[:5])))
        else:
            log(t("dl_analyze_pass", len(expected_indices)))
    else:
        # æ²¡æœ‰æœŸæœ›åˆ—è¡¨ï¼Œä½¿ç”¨å·²ä¸‹è½½å†…å®¹åˆ†æ
        result['total_expected'] = len(chapter_results)
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦è¿ç»­
        sorted_indices = sorted(downloaded_indices)
        if sorted_indices:
            min_idx, max_idx = sorted_indices[0], sorted_indices[-1]
            expected_range = set(range(min_idx, max_idx + 1))
            missing_in_range = expected_range - downloaded_indices
            
            if missing_in_range:
                result['missing_indices'] = sorted(list(missing_in_range))
                log(t("dl_analyze_gap", sorted(missing_in_range)[:10]))
    
    # éªŒè¯ç« èŠ‚é¡ºåºï¼ˆæ£€æŸ¥æ ‡é¢˜ä¸­çš„ç« èŠ‚å·æ˜¯å¦é€’å¢ï¼‰
    sorted_results = sorted(chapter_results.items(), key=lambda x: x[0])
    order_issues = []
    
    for i in range(1, len(sorted_results)):
        prev_idx, prev_data = sorted_results[i-1]
        curr_idx, curr_data = sorted_results[i]
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦è¿ç»­
        if curr_idx != prev_idx + 1:
            order_issues.append({
                'type': 'gap',
                'from_index': prev_idx,
                'to_index': curr_idx,
                'gap': curr_idx - prev_idx - 1
            })
    
    if order_issues:
        result['order_correct'] = False
        total_gaps = sum(issue['gap'] for issue in order_issues)
        log(t("dl_analyze_order_fail", len(order_issues), total_gaps))
    else:
        log(t("dl_analyze_order_pass"))
    
    # è®¡ç®—å®Œæ•´åº¦
    if result['total_expected'] > 0:
        result['completeness_percent'] = (result['total_downloaded'] / result['total_expected']) * 100
    
    return result


def download_cover(cover_url, headers):
    """ä¸‹è½½å°é¢å›¾ç‰‡"""
    if not cover_url:
        return None, None, None
    
    try:
        response = requests.get(cover_url, headers=headers, timeout=15)
        if response.status_code != 200:
            return None, None, None
        
        content_type = response.headers.get('content-type', '')
        content_bytes = response.content
        
        if len(content_bytes) < 1000:
            return None, None, None
        
        if 'jpeg' in content_type or 'jpg' in content_type:
            file_ext, mime_type = '.jpg', 'image/jpeg'
        elif 'png' in content_type:
            file_ext, mime_type = '.png', 'image/png'
        elif 'webp' in content_type:
            file_ext, mime_type = '.webp', 'image/webp'
        else:
            file_ext, mime_type = '.jpg', 'image/jpeg'
        
        return content_bytes, file_ext, mime_type
        
    except Exception as e:
        with print_lock:
            print(t("dl_cover_fail", str(e)))
        return None, None, None


def create_epub(name, author_name, description, cover_url, chapters, save_path):
    """åˆ›å»ºEPUBæ–‡ä»¶"""
    book = epub.EpubBook()
    book.set_identifier(f'fanqie_{int(time.time())}')
    book.set_title(name)
    book.set_language('zh-CN')
    
    if author_name:
        book.add_author(author_name)
    
    if description:
        book.add_metadata('DC', 'description', description)
    
    if cover_url:
        try:
            cover_content, file_ext, mime_type = download_cover(cover_url, get_headers())
            if cover_content and file_ext and mime_type:
                book.set_cover(f'cover{file_ext}', cover_content)
        except Exception as e:
            with print_lock:
                print(t("dl_cover_add_fail", str(e)))
    
    spine_items = ['nav']
    toc_items = []
    
    # åˆ›å»ºä¹¦ç±ä¿¡æ¯é¡µ (ç®€ä»‹é¡µ)
    intro_html = f'<h1>{name}</h1>'
    if author_name:
        intro_html += f'<p><strong>ä½œè€…ï¼š</strong> {author_name}</p>'
    
    if description:
        intro_html += '<hr/>'
        intro_html += f'<h3>{t("dl_intro_title")}</h3>'
        # å¤„ç†ç®€ä»‹çš„æ¢è¡Œ
        desc_lines = description.split('\n')
        for line in desc_lines:
            if line.strip():
                intro_html += f'<p>{line.strip()}</p>'
                
    intro_chapter = epub.EpubHtml(title=t('dl_book_detail_title'), file_name='intro.xhtml', lang='zh-CN')
    intro_chapter.content = intro_html
    book.add_item(intro_chapter)
    
    # å°†ç®€ä»‹é¡µæ·»åŠ åˆ° spine å’Œ toc
    spine_items.append(intro_chapter)
    toc_items.append(intro_chapter)

    for idx, ch_data in enumerate(chapters):
        chapter_file = f'chapter_{idx + 1}.xhtml'
        title = ch_data.get('title', f'ç¬¬{idx + 1}ç« ')
        content = ch_data.get('content', '')
        
        # å°†æ¢è¡Œç¬¦è½¬æ¢ä¸ºHTMLæ®µè½æ ‡ç­¾
        paragraphs = content.split('\n\n') if content else []
        html_paragraphs = ''.join(f'<p>{p.strip()}</p>' for p in paragraphs if p.strip())
        
        chapter = epub.EpubHtml(
            title=title,
            file_name=chapter_file,
            lang='zh-CN'
        )
        chapter.content = f'<h1>{title}</h1><div>{html_paragraphs}</div>'
        
        book.add_item(chapter)
        spine_items.append(chapter)
        toc_items.append(chapter)
    
    book.toc = toc_items
    book.spine = spine_items
    
    book.add_item(epub.EpubNcx())
    book.add_item(epub.EpubNav())
    
    # ä½¿ç”¨æ–°çš„æ–‡ä»¶å‘½åé€»è¾‘
    filename = generate_filename(name, author_name, 'epub')
    epub_path = os.path.join(save_path, filename)
    epub.write_epub(epub_path, book)
    
    return epub_path


def create_txt(name, author_name, description, chapters, save_path):
    """åˆ›å»ºTXTæ–‡ä»¶"""
    # ä½¿ç”¨æ–°çš„æ–‡ä»¶å‘½åé€»è¾‘
    filename = generate_filename(name, author_name, 'txt')
    txt_path = os.path.join(save_path, filename)
    
    with open(txt_path, 'w', encoding='utf-8') as f:
        f.write(f"{name}\n")
        if author_name:
            f.write(f"{t('label_author')}{author_name}\n")
        if description:
            f.write(f"\n{t('dl_intro_title')}:\n{description}\n")
        f.write("\n" + "="*50 + "\n\n")
        
        for ch_data in chapters:
            title = ch_data.get('title', '')
            content = ch_data.get('content', '')
            f.write(f"\n{title}\n\n")
            f.write(f"{content}\n\n")
    
    return txt_path


def Run(book_id, save_path, file_format='txt', start_chapter=None, end_chapter=None, selected_chapters=None, gui_callback=None):
    """è¿è¡Œä¸‹è½½ - ä¼˜åŒ–ç‰ˆï¼šå‡å°‘åˆå§‹åŒ–æ—¶é—´"""

    api = get_api_manager()
    if api is None:
        return False

    def log_message(message, progress=-1):
        if gui_callback and len(inspect.signature(gui_callback).parameters) > 1:
            gui_callback(progress, message)
        else:
            print(message)

    async def async_download_flow():
        """å¼‚æ­¥ä¸‹è½½æµç¨‹ï¼Œå‡å°‘åˆå§‹åŒ–å»¶è¿Ÿ"""
        try:
            # é¢„åˆå§‹åŒ–å¼‚æ­¥ä¼šè¯
            log_message("æ­£åœ¨åˆå§‹åŒ–ä¸‹è½½å™¨...", 2)
            await api.pre_initialize()

            log_message(t("dl_fetching_info"), 5)
            book_detail = api.get_book_detail(book_id)
            if not book_detail:
                log_message(t("dl_fetch_info_fail"))
                return False

            name = book_detail.get("book_name", f"æœªçŸ¥å°è¯´_{book_id}")
            author_name = book_detail.get("author", t("dl_unknown_author"))
            description = book_detail.get("abstract", "")
            cover_url = book_detail.get("thumb_url", "")

            log_message(t("dl_book_info_log", name, author_name), 10)

            chapter_results = {}
            use_full_download = False
            speed_mode_downloaded_ids = set()

            # å¹¶è¡Œè·å–ç« èŠ‚ç›®å½•ï¼ˆä½¿ç”¨å¼‚æ­¥æ–¹å¼ï¼‰
            log_message("æ­£åœ¨è·å–ç« èŠ‚åˆ—è¡¨...", 15)
            chapters = []

            # å¹¶è¡Œå°è¯•ä¸¤ä¸ªæ¥å£
            directory_task = asyncio.create_task(api.get_directory_async(book_id))
            chapter_list_task = asyncio.create_task(api.get_chapter_list_async(book_id))

            # ç­‰å¾…directoryæ¥å£ç»“æœ
            directory_data = await directory_task
            if directory_data:
                for idx, ch in enumerate(directory_data):
                    item_id = ch.get("item_id")
                    title = ch.get("title", f"ç¬¬{idx+1}ç« ")
                    if item_id:
                        chapters.append({"id": str(item_id), "title": title, "index": idx})

            # å¦‚æœdirectoryå¤±è´¥ï¼Œä½¿ç”¨chapter_listç»“æœ
            if not chapters:
                chapters_data = await chapter_list_task
                if chapters_data:
                    if isinstance(chapters_data, dict):
                        all_item_ids = chapters_data.get("allItemIds", [])
                        chapter_list = chapters_data.get("chapterListWithVolume", [])

                        if chapter_list:
                            idx = 0
                            for volume in chapter_list:
                                if isinstance(volume, list):
                                    for ch in volume:
                                        if isinstance(ch, dict):
                                            item_id = ch.get("itemId") or ch.get("item_id")
                                            title = ch.get("title", f"ç¬¬{idx+1}ç« ")
                                            if item_id:
                                                chapters.append({"id": str(item_id), "title": title, "index": idx})
                                                idx += 1
                        else:
                            for idx, item_id in enumerate(all_item_ids):
                                chapters.append({"id": str(item_id), "title": f"ç¬¬{idx+1}ç« ", "index": idx})
                    elif isinstance(chapters_data, list):
                        for idx, ch in enumerate(chapters_data):
                            item_id = ch.get("item_id") or ch.get("chapter_id")
                            title = ch.get("title", f"ç¬¬{idx+1}ç« ")
                            if item_id:
                                chapters.append({"id": str(item_id), "title": title, "index": idx})

            if not chapters:
                log_message(t("dl_fetch_list_fail"))
                return False

            total_chapters = len(chapters)
            log_message(t("dl_found_chapters", total_chapters), 20)

            # å°è¯•æé€Ÿä¸‹è½½æ¨¡å¼ (ä»…å½“æ²¡æœ‰æŒ‡å®šèŒƒå›´ä¸”æ²¡æœ‰é€‰æ‹©ç‰¹å®šç« èŠ‚æ—¶)
            if start_chapter is None and end_chapter is None and not selected_chapters:
                log_message(t("dl_try_speed_mode"), 25)
                full_content = api.get_full_content(book_id)
                if full_content:
                    log_message(t("dl_speed_mode_success"), 30)
                    # æ‰¹é‡æ¨¡å¼ï¼šè¿”å› {item_id: content}ï¼Œå¯ç²¾å‡†ä¸ç›®å½•å¯¹é½
                    if isinstance(full_content, dict):
                        with tqdm(total=len(chapters), desc=t("dl_processing_chapters"), disable=gui_callback is not None) as pbar:
                            for ch in chapters:
                                raw = full_content.get(ch['id'])
                                if isinstance(raw, str) and raw.strip():
                                    processed = process_chapter_content(raw)
                                    chapter_results[ch['index']] = {
                                        'title': ch['title'],
                                        'content': processed
                                    }
                                    speed_mode_downloaded_ids.add(ch['id'])
                                if pbar:
                                    pbar.update(1)

                        parsed_count = len(speed_mode_downloaded_ids)
                        log_message(t("dl_speed_mode_parsed", parsed_count), 50)

                        if parsed_count == total_chapters:
                            use_full_download = True
                            log_message(t("dl_process_complete"), 80)
                        else:
                            log_message(f"æ€¥é€Ÿæ¨¡å¼æ‰¹é‡å†…å®¹ä¸å®Œæ•´ ({parsed_count}/{total_chapters})ï¼Œå°†ç¼ºå¤±ç« èŠ‚åˆ‡æ¢åˆ°æ™®é€šæ¨¡å¼ä¸‹è½½")
                    else:
                        full_text = str(full_content)
                        # ä½¿ç”¨ç›®å½•æ ‡é¢˜æ¥åˆ†å‰²å†…å®¹ï¼ˆå…¼å®¹æ—§èŠ‚ç‚¹/ä¸‹è½½æ¨¡å¼ï¼‰
                        chapters_parsed = parse_novel_text_with_catalog(full_text, chapters)

                        if chapters_parsed and len(chapters_parsed) >= len(chapters) * 0.8:
                            # æˆåŠŸè§£æå‡ºè‡³å°‘80%çš„ç« èŠ‚
                            log_message(t("dl_speed_mode_parsed", len(chapters_parsed)), 50)
                            with tqdm(total=len(chapters_parsed), desc=t("dl_processing_chapters"), disable=gui_callback is not None) as pbar:
                                for ch in chapters_parsed:
                                    processed = process_chapter_content(ch['content'])
                                    chapter_results[ch['index']] = {
                                        'title': ch['title'],
                                        'content': processed
                                    }
                                    if pbar:
                                        pbar.update(1)

                            use_full_download = True
                            log_message(t("dl_process_complete"), 80)
                        else:
                            parsed_count = len(chapters_parsed) if chapters_parsed else 0
                            log_message(f"æ€¥é€Ÿæ¨¡å¼è§£æä¸å®Œæ•´ ({parsed_count}/{total_chapters})ï¼Œåˆ‡æ¢åˆ°æ™®é€šæ¨¡å¼")
                else:
                    log_message(t("dl_speed_mode_fail"))

            # å¦‚æœæ²¡æœ‰ä½¿ç”¨æé€Ÿæ¨¡å¼ï¼Œåˆ™èµ°ä¼˜åŒ–çš„å¼‚æ­¥æ¨¡å¼
            if not use_full_download:

                if not chapters:
                    log_message(t("dl_no_chapters_found"))
                    return False

                total_chapters = len(chapters)
                log_message(t("dl_found_chapters", total_chapters), 20)

                if start_chapter is not None or end_chapter is not None:
                    start_idx = (start_chapter - 1) if start_chapter else 0
                    end_idx = end_chapter if end_chapter else total_chapters
                    chapters = chapters[start_idx:end_idx]
                    log_message(t("dl_range_log", start_idx+1, end_idx))

                if selected_chapters:
                    try:
                        selected_indices = set(int(x) for x in selected_chapters)
                        chapters = [ch for ch in chapters if ch['index'] in selected_indices]
                        log_message(t("dl_selected_log", len(chapters)))
                    except Exception as e:
                        log_message(t("dl_filter_error", e))

                downloaded_ids = load_status(book_id)
                if speed_mode_downloaded_ids:
                    downloaded_ids.update(speed_mode_downloaded_ids)

                # åŠ è½½å·²ä¿å­˜çš„ç« èŠ‚å†…å®¹ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
                saved_content = load_saved_content(book_id)
                if saved_content:
                    log_message(f"å‘ç°å·²ä¿å­˜çš„ä¸‹è½½è¿›åº¦ï¼Œå·²æœ‰ {len(saved_content)} ä¸ªç« èŠ‚", 22)
                    chapter_results.update(saved_content)

                chapters_to_download = [ch for ch in chapters if ch["id"] not in downloaded_ids]

                if not chapters_to_download:
                    log_message(t("dl_all_downloaded"))
                else:
                    log_message(t("dl_start_download_log", len(chapters_to_download)), 25)

                # ä½¿ç”¨ä¼˜åŒ–çš„å¼‚æ­¥ä¸‹è½½æ›¿ä»£ThreadPoolExecutor
                if chapters_to_download:
                    def progress_callback(progress, message):
                        if gui_callback:
                            gui_callback(25 + int(progress * 0.6), message)

                    # ç›´æ¥å¤ç”¨åŒä¸€ä¸ª API å®ä¾‹çš„å¼‚æ­¥èƒ½åŠ›ï¼Œé¿å…å¤åˆ¶/å…±äº«å†…éƒ¨ä¼šè¯çŠ¶æ€
                    if hasattr(api, 'download_chapters_async'):
                        async_results = await api.download_chapters_async(chapters_to_download, progress_callback)
                    else:
                        api_ext = APIManagerExt()
                        api_ext.base_url = api.base_url
                        api_ext.endpoints = api.endpoints
                        async_results = await api_ext.download_chapters_async(chapters_to_download, progress_callback)

                    # åˆå¹¶ç»“æœ
                    for idx, data in async_results.items():
                        chapter_results[idx] = data
                        # æ‰¾åˆ°å¯¹åº”çš„ç« èŠ‚IDå¹¶æ ‡è®°ä¸ºå·²ä¸‹è½½
                        for ch in chapters_to_download:
                            if ch['index'] == idx:
                                downloaded_ids.add(ch['id'])
                                break

                # ä¿å­˜ä¸‹è½½çŠ¶æ€å’Œç« èŠ‚å†…å®¹
                save_status(book_id, downloaded_ids)
                save_content(book_id, chapter_results)

            # å…¶ä½™å¤„ç†é€»è¾‘ä¿æŒä¸å˜...
            # ==================== ä¸‹è½½å®Œæ•´æ€§åˆ†æ ====================
            if gui_callback:
                gui_callback(85, t("dl_analyzing_completeness"))
            else:
                log_message(t("dl_analyzing_completeness"), 85)

            # åˆ†æç»“æœ
            analysis_result = analyze_download_completeness(
                chapter_results,
                chapters if not use_full_download else None,
                log_message
            )

            # å¦‚æœæœ‰ç¼ºå¤±ç« èŠ‚ï¼Œå°è¯•è¡¥å……ä¸‹è½½
            if analysis_result['missing_indices'] and not use_full_download:
                missing_count = len(analysis_result['missing_indices'])
                log_message(t("dl_missing_retry", missing_count), 87)

                # è·å–ç¼ºå¤±ç« èŠ‚çš„ä¿¡æ¯
                missing_chapters = [ch for ch in chapters if ch['index'] in analysis_result['missing_indices']]

                # è¡¥å……ä¸‹è½½ç¼ºå¤±ç« èŠ‚ï¼ˆæœ€å¤šé‡è¯•3æ¬¡ï¼‰
                for retry in range(3):
                    if not missing_chapters:
                        break

                    log_message(t("dl_retry_log", retry + 1, len(missing_chapters)), 88)
                    still_missing = []

                    for ch in missing_chapters:
                        try:
                            data = api.get_chapter_content(ch["id"])
                            if data and data.get('content'):
                                processed = process_chapter_content(data.get('content', ''))
                                chapter_results[ch['index']] = {
                                    'title': ch['title'],
                                    'content': processed
                                }
                                downloaded_ids.add(ch['id'])
                            else:
                                still_missing.append(ch)
                        except Exception:
                            still_missing.append(ch)
                        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«

                    missing_chapters = still_missing
                    if not missing_chapters:
                        log_message(t("dl_retry_success"), 90)
                        break

                # æ›´æ–°çŠ¶æ€
                save_status(book_id, downloaded_ids)

                # æœ€ç»ˆæ£€æŸ¥
                if missing_chapters:
                    missing_indices = [ch['index'] + 1 for ch in missing_chapters]
                    log_message(t("dl_retry_fail", len(missing_chapters), missing_indices[:10]), 90)

            # éªŒè¯ç« èŠ‚é¡ºåºï¼ˆä½¿ç”¨ ChapterOrderValidatorï¼‰
            if gui_callback:
                gui_callback(92, t("dl_verifying_order"))

            # åˆ›å»ºéªŒè¯å™¨å®ä¾‹
            order_validator = ChapterOrderValidator(chapters)

            # éªŒè¯é¡ºåº
            validation_result = order_validator.validate_order(chapter_results)
            sequential_result = order_validator.verify_sequential(chapter_results)

            if not validation_result['is_valid']:
                if validation_result['gaps']:
                    log_message(f"æ£€æµ‹åˆ°ç¼ºå¤±ç« èŠ‚: {len(validation_result['gaps'])} ä¸ª", 93)
                if validation_result['out_of_order']:
                    issues_preview = validation_result['out_of_order'][:5]
                    log_message(f"æ£€æµ‹åˆ°ç« èŠ‚åºå·ä¸è¿ç»­: {issues_preview}{'...' if len(validation_result['out_of_order']) > 5 else ''}", 93)
            else:
                log_message("ç« èŠ‚é¡ºåºéªŒè¯é€šè¿‡", 93)

            # ä½¿ç”¨éªŒè¯å™¨æ’åºç« èŠ‚
            sorted_chapters = order_validator.sort_chapters(chapter_results)

            # æœ€ç»ˆç»Ÿè®¡
            total_expected = len(chapters) if not use_full_download else len(chapter_results)
            total_downloaded = len(chapter_results)
            completeness = (total_downloaded / total_expected * 100) if total_expected > 0 else 100

            log_message(f"ä¸‹è½½ç»Ÿè®¡: {total_downloaded}/{total_expected} ç«  ({completeness:.1f}%)", 95)

            if gui_callback:
                gui_callback(95, "æ­£åœ¨ç”Ÿæˆæ–‡ä»¶...")

            if file_format == 'epub':
                output_file = create_epub(name, author_name, description, cover_url, sorted_chapters, save_path)
            else:
                output_file = create_txt(name, author_name, description, sorted_chapters, save_path)

            # ä¸‹è½½å®Œæˆåæ¸…é™¤ä¸´æ—¶çŠ¶æ€æ–‡ä»¶
            clear_status(book_id)

            # æœ€ç»ˆç»“æœ
            if completeness >= 100:
                log_message(f"ä¸‹è½½å®Œæˆ! æ–‡ä»¶: {output_file}", 100)
            else:
                log_message(f"ä¸‹è½½å®Œæˆ(éƒ¨åˆ†ç« èŠ‚ç¼ºå¤±)! æ–‡ä»¶: {output_file}", 100)

            return True

        except Exception as e:
            log_message(f"ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
        finally:
            # æ¸…ç†å¼‚æ­¥ä¼šè¯
            try:
                await api.close_async()
            except Exception:
                pass

    # è¿è¡Œå¼‚æ­¥ä¸‹è½½æµç¨‹ - æ‰“åŒ…ç¯å¢ƒå…¼å®¹ç‰ˆ
    try:
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰äº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_running_loop()
            # å¦‚æœå·²æœ‰è¿è¡Œä¸­çš„å¾ªç¯ï¼Œåˆ›å»ºä»»åŠ¡
            task = loop.create_task(async_download_flow())
            return asyncio.run_coroutine_threadsafe(task, loop).result()
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
            if sys.platform == 'win32' and getattr(sys, 'frozen', False):
                # Windowsæ‰“åŒ…ç¯å¢ƒç‰¹æ®Šå¤„ç†
                try:
                    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
                except AttributeError:
                    pass

            return asyncio.run(async_download_flow())
    except Exception as e:
        log_message(f"ä¸‹è½½å¤±è´¥: {str(e)}")
        return False


# ===================== ç« èŠ‚é¡ºåºéªŒè¯å™¨ =====================

class ChapterOrderValidator:
    """éªŒè¯å’Œä¿®å¤ç« èŠ‚é¡ºåº
    
    ç¡®ä¿ä¸‹è½½çš„ç« èŠ‚æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—ï¼Œæ£€æµ‹ç¼ºå¤±å’Œé‡å¤
    """
    
    def __init__(self, expected_chapters: List[dict]):
        """
        Args:
            expected_chapters: æœŸæœ›çš„ç« èŠ‚åˆ—è¡¨ [{'id': str, 'title': str, 'index': int}, ...]
        """
        self.expected_chapters = expected_chapters
        self.chapter_map = {str(ch.get('id', ch.get('item_id', ''))): ch.get('index', i) 
                          for i, ch in enumerate(expected_chapters)}
        self.index_to_chapter = {ch.get('index', i): ch for i, ch in enumerate(expected_chapters)}
    
    def validate_order(self, chapter_results: dict) -> dict:
        """
        éªŒè¯ç« èŠ‚é¡ºåº
        
        Args:
            chapter_results: ä¸‹è½½ç»“æœ {index: {'title': str, 'content': str}, ...}
        
        Returns:
            {
                'is_valid': bool,
                'gaps': List[int],      # ç¼ºå¤±çš„ç« èŠ‚ç´¢å¼•
                'out_of_order': List[tuple],  # é¡ºåºé”™è¯¯çš„ç« èŠ‚å¯¹
                'duplicates': List[int]  # é‡å¤çš„ç« èŠ‚ç´¢å¼•
            }
        """
        result = {
            'is_valid': True,
            'gaps': [],
            'out_of_order': [],
            'duplicates': []
        }
        
        if not chapter_results:
            return result
        
        # è·å–æ‰€æœ‰ç´¢å¼•å¹¶æ’åº
        indices = sorted(chapter_results.keys())
        
        if not indices:
            return result
        
        # æ£€æŸ¥ç¼ºå¤±çš„ç« èŠ‚ï¼ˆåœ¨æœŸæœ›èŒƒå›´å†…ï¼‰
        expected_indices = set(range(len(self.expected_chapters)))
        downloaded_indices = set(indices)
        result['gaps'] = sorted(list(expected_indices - downloaded_indices))
        
        # æ£€æŸ¥é¡ºåºæ˜¯å¦æ­£ç¡®ï¼ˆç´¢å¼•åº”è¯¥æ˜¯è¿ç»­é€’å¢çš„ï¼‰
        for i in range(1, len(indices)):
            if indices[i] != indices[i-1] + 1:
                # å‘ç°ä¸è¿ç»­
                result['out_of_order'].append((indices[i-1], indices[i]))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆ
        if result['gaps'] or result['out_of_order'] or result['duplicates']:
            result['is_valid'] = False
        
        return result
    
    def sort_chapters(self, chapter_results: dict) -> List[dict]:
        """
        æŒ‰æ­£ç¡®é¡ºåºæ’åºç« èŠ‚

        Args:
            chapter_results: ä¸‹è½½ç»“æœ {index: {'title': str, 'content': str}, ...}

        Returns:
            æ’åºåçš„ç« èŠ‚åˆ—è¡¨ [{'index': int, 'title': str, 'content': str}, ...]
        """
        sorted_chapters = []

        # ç¡®ä¿ key æ˜¯æ•´æ•°ç±»å‹åæ’åº
        int_keys = []
        for k in chapter_results.keys():
            try:
                int_keys.append(int(k))
            except (ValueError, TypeError):
                # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•´æ•°ï¼Œè·³è¿‡
                continue

        int_keys.sort()

        for index in int_keys:
            chapter_data = chapter_results.get(index) or chapter_results.get(str(index))
            if chapter_data:
                sorted_chapters.append({
                    'index': index,
                    'title': chapter_data.get('title', f'ç¬¬{index + 1}ç« '),
                    'content': chapter_data.get('content', '')
                })

        return sorted_chapters
    
    def map_bulk_content(self, bulk_data: dict, item_ids: List[str]) -> dict:
        """
        å°†æ‰¹é‡ä¸‹è½½å†…å®¹æ˜ å°„åˆ°æ­£ç¡®çš„ç« èŠ‚ç´¢å¼•
        
        Args:
            bulk_data: æ‰¹é‡ä¸‹è½½çš„åŸå§‹æ•°æ® {item_id: content, ...}
            item_ids: ç« èŠ‚IDåˆ—è¡¨ï¼ˆæŒ‰ç›®å½•é¡ºåºï¼‰
        
        Returns:
            æ˜ å°„åçš„ç»“æœ {index: {'title': str, 'content': str}, ...}
        """
        result = {}
        
        for idx, item_id in enumerate(item_ids):
            item_id_str = str(item_id)
            if item_id_str in bulk_data:
                content_data = bulk_data[item_id_str]
                if isinstance(content_data, dict):
                    result[idx] = {
                        'title': content_data.get('title', f'ç¬¬{idx + 1}ç« '),
                        'content': content_data.get('content', '')
                    }
                else:
                    result[idx] = {
                        'title': f'ç¬¬{idx + 1}ç« ',
                        'content': str(content_data)
                    }
        
        return result
    
    def verify_sequential(self, chapter_results: dict) -> dict:
        """
        éªŒè¯ç« èŠ‚ç´¢å¼•æ˜¯å¦è¿ç»­æ— é—´éš™
        
        Args:
            chapter_results: ä¸‹è½½ç»“æœ
        
        Returns:
            {
                'is_sequential': bool,
                'missing_count': int,
                'missing_indices': List[int]
            }
        """
        if not chapter_results:
            return {'is_sequential': True, 'missing_count': 0, 'missing_indices': []}
        
        indices = sorted(chapter_results.keys())
        min_idx, max_idx = indices[0], indices[-1]
        
        expected_set = set(range(min_idx, max_idx + 1))
        actual_set = set(indices)
        missing = sorted(list(expected_set - actual_set))
        
        return {
            'is_sequential': len(missing) == 0,
            'missing_count': len(missing),
            'missing_indices': missing
        }
    
    def map_text_parsed_content(self, parsed_chapters: List[dict], catalog: List[dict]) -> dict:
        """
        å°†æ–‡æœ¬è§£ææ¨¡å¼çš„ç« èŠ‚å†…å®¹æ˜ å°„åˆ°æ­£ç¡®çš„ç´¢å¼•
        
        ä½¿ç”¨ç›®å½•ä¸­çš„ç« èŠ‚æ ‡é¢˜æ¥åŒ¹é…è§£æå‡ºçš„ç« èŠ‚ï¼Œç¡®ä¿é¡ºåºæ­£ç¡®
        
        Args:
            parsed_chapters: è§£æå‡ºçš„ç« èŠ‚åˆ—è¡¨ [{'title': str, 'content': str}, ...]
            catalog: ç›®å½•ç« èŠ‚åˆ—è¡¨ [{'id': str, 'title': str, 'index': int}, ...]
        
        Returns:
            æ˜ å°„åçš„ç»“æœ {index: {'title': str, 'content': str}, ...}
        """
        result = {}
        
        # æ„å»ºæ ‡é¢˜åˆ°ç›®å½•ç´¢å¼•çš„æ˜ å°„
        title_to_index = {}
        for ch in catalog:
            # æ ‡å‡†åŒ–æ ‡é¢˜ï¼ˆå»é™¤ç©ºç™½ã€ç»Ÿä¸€æ ¼å¼ï¼‰
            normalized_title = ch.get('title', '').strip()
            title_to_index[normalized_title] = ch.get('index', 0)
        
        # æ˜ å°„è§£æå‡ºçš„ç« èŠ‚
        for parsed_ch in parsed_chapters:
            parsed_title = parsed_ch.get('title', '').strip()
            
            # å°è¯•ç²¾ç¡®åŒ¹é…
            if parsed_title in title_to_index:
                idx = title_to_index[parsed_title]
                result[idx] = {
                    'title': parsed_title,
                    'content': parsed_ch.get('content', '')
                }
            else:
                # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ï¼‰
                import re
                clean_parsed = re.sub(r'[\s\u3000]+', '', parsed_title)
                for cat_title, idx in title_to_index.items():
                    clean_cat = re.sub(r'[\s\u3000]+', '', cat_title)
                    if clean_parsed == clean_cat:
                        result[idx] = {
                            'title': cat_title,  # ä½¿ç”¨ç›®å½•ä¸­çš„æ ‡å‡†æ ‡é¢˜
                            'content': parsed_ch.get('content', '')
                        }
                        break
        
        return result
    
    def get_validation_summary(self, chapter_results: dict) -> str:
        """
        è·å–éªŒè¯ç»“æœçš„æ‘˜è¦ä¿¡æ¯
        
        Args:
            chapter_results: ä¸‹è½½ç»“æœ
        
        Returns:
            æ‘˜è¦å­—ç¬¦ä¸²
        """
        validation = self.validate_order(chapter_results)
        sequential = self.verify_sequential(chapter_results)
        
        lines = []
        
        if validation['is_valid'] and sequential['is_sequential']:
            lines.append("âœ“ ç« èŠ‚é¡ºåºéªŒè¯é€šè¿‡")
        else:
            if validation['gaps']:
                lines.append(f"âš  ç¼ºå¤±ç« èŠ‚: {len(validation['gaps'])} ä¸ª")
            if validation['out_of_order']:
                lines.append(f"âš  é¡ºåºå¼‚å¸¸: {len(validation['out_of_order'])} å¤„")
            if sequential['missing_indices']:
                lines.append(f"âš  ç´¢å¼•ä¸è¿ç»­: ç¼ºå¤± {sequential['missing_count']} ä¸ª")
        
        return '\n'.join(lines) if lines else "ç« èŠ‚é¡ºåºæ­£å¸¸"


class NovelDownloader:
    """å°è¯´ä¸‹è½½å™¨ç±»"""
    
    def __init__(self):
        self.is_cancelled = False
        self.current_progress_callback = None
        self.gui_verification_callback = None
    
    def cancel_download(self):
        """å–æ¶ˆä¸‹è½½"""
        self.is_cancelled = True
    
    def run_download(self, book_id, save_path, file_format='txt', start_chapter=None, end_chapter=None, selected_chapters=None, gui_callback=None):
        """è¿è¡Œä¸‹è½½"""
        try:
            if gui_callback:
                self.gui_verification_callback = gui_callback
            
            return Run(book_id, save_path, file_format, start_chapter, end_chapter, selected_chapters, gui_callback)
        except Exception as e:
            print(f"ä¸‹è½½å¤±è´¥: {str(e)}")
            return False
    
    def search_novels(self, keyword, offset=0):
        """æœç´¢å°è¯´"""
        try:
            api = get_api_manager()
            if api is None:
                return None
            
            search_results = api.search_books(keyword, offset)
            if search_results and search_results.get("data"):
                return search_results["data"]
            return None
        except Exception as e:
            with print_lock:
                print(t("dl_search_fail", str(e)))
            return None


downloader_instance = NovelDownloader()


class BatchDownloader:
    """æ‰¹é‡ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.is_cancelled = False
        self.results = []  # ä¸‹è½½ç»“æœåˆ—è¡¨
        self.current_index = 0
        self.total_count = 0
    
    def cancel(self):
        """å–æ¶ˆæ‰¹é‡ä¸‹è½½"""
        self.is_cancelled = True
    
    def reset(self):
        """é‡ç½®çŠ¶æ€"""
        self.is_cancelled = False
        self.results = []
        self.current_index = 0
        self.total_count = 0
    
    def run_batch(
        self,
        book_ids: list,
        save_path: str,
        file_format: str = 'txt',
        progress_callback=None,
        delay_between_books: float = 2.0,
        max_concurrent: int = 1,
        log_func=None,
    ):
        """
        æ‰¹é‡ä¸‹è½½å¤šæœ¬ä¹¦ç±
        
        Args:
            book_ids: ä¹¦ç±IDåˆ—è¡¨
            save_path: ä¿å­˜è·¯å¾„
            file_format: æ–‡ä»¶æ ¼å¼ ('txt' æˆ– 'epub')
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, book_name, status, message)
            delay_between_books: æ¯æœ¬ä¹¦ä¹‹é—´çš„å»¶è¿Ÿï¼ˆç§’ï¼‰
            max_concurrent: å¹¶å‘ä¸‹è½½æ•°é‡ï¼ˆ>=1ï¼‰ã€‚ä¸ºä¿è¯ç¨³å®šæ€§ä¼šå¼ºåˆ¶é™åˆ¶æœ€å¤§å€¼ã€‚
            log_func: æ—¥å¿—è¾“å‡ºå‡½æ•°ï¼ŒNone è¡¨ç¤ºä¸è¾“å‡ºï¼ˆé»˜è®¤ï¼šæ—  progress_callback æ—¶è¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰
        
        Returns:
            dict: æ‰¹é‡ä¸‹è½½ç»“æœ
        """
        from datetime import datetime

        self.reset()
        self.total_count = len(book_ids)
        
        if not book_ids:
            return {'success': False, 'message': t('dl_batch_no_books'), 'results': []}
        
        api = get_api_manager()
        if api is None:
            return {'success': False, 'message': t('dl_batch_api_fail'), 'results': []}
        
        # é»˜è®¤æ—¥å¿—ç­–ç•¥ï¼šåªæœ‰åœ¨é GUI/å›è°ƒæ¨¡å¼ä¸‹æ‰è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œé¿å…æ±¡æŸ“ Web ç«¯æ—¥å¿—
        if log_func is None and progress_callback is None:
            log_func = print

        def log(msg: str) -> None:
            if not log_func:
                return
            try:
                with print_lock:
                    log_func(msg)
            except Exception:
                try:
                    log_func(msg)
                except Exception:
                    pass

        def safe_progress(current, total, book_name, status, message) -> None:
            if not progress_callback:
                return
            try:
                progress_callback(current, total, book_name, status, message)
            except Exception:
                pass

        # å¹¶å‘é™åˆ¶ï¼ˆé»˜è®¤ä¸ CLI ä¸€è‡´ï¼Œé¿å…åˆ›å»ºè¿‡å¤šçº¿ç¨‹/è¯·æ±‚è¿‡å¿«ï¼‰
        try:
            max_concurrent = int(max_concurrent or 1)
        except Exception:
            max_concurrent = 1
        max_concurrent = max(1, max_concurrent)
        max_concurrent = min(max_concurrent, 5)
        max_concurrent = min(max_concurrent, len(book_ids))

        def get_book_name(book_id: str) -> str:
            book_name = f"ä¹¦ç±_{book_id}"
            try:
                book_detail = api.get_book_detail(book_id)
                if isinstance(book_detail, dict) and not book_detail.get('_error'):
                    book_name = book_detail.get('book_name', book_name)
            except Exception:
                pass
            return book_name

        def download_single_book(book_id: str, index: int) -> dict:
            """ä¸‹è½½å•æœ¬ä¹¦ç±ï¼ˆæ”¯æŒå¹¶å‘æ‰§è¡Œï¼‰"""
            if self.is_cancelled:
                return {
                    'book_id': str(book_id).strip(),
                    'book_name': f"ä¹¦ç±_{str(book_id).strip()}",
                    'success': False,
                    'message': t("dl_batch_cancelled"),
                    'index': index,
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }

            book_id = str(book_id).strip()
            book_name = get_book_name(book_id)
            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            log("\n" + t("dl_batch_downloading", index, self.total_count, book_name))
            safe_progress(index, self.total_count, book_name, 'downloading', t("dl_batch_progress", index))

            result = {
                'book_id': book_id,
                'book_name': book_name,
                'success': False,
                'message': '',
                'index': index,
                'timestamp': timestamp
            }

            try:
                def single_book_callback(progress, message):
                    safe_progress(index, self.total_count, book_name, 'downloading', message)

                success = Run(book_id, save_path, file_format, gui_callback=single_book_callback)

                if success:
                    result['success'] = True
                    result['message'] = 'ä¸‹è½½æˆåŠŸ'
                    log(t("dl_batch_success", book_name))
                else:
                    result['message'] = 'ä¸‹è½½å¤±è´¥'
                    log(t("dl_batch_fail", book_name))

            except Exception as e:
                result['message'] = str(e)
                log(t("dl_batch_exception", book_name, str(e)))

            status = 'success' if result['success'] else 'failed'
            safe_progress(index, self.total_count, book_name, status, result['message'])
            return result

        log(t("dl_batch_start", self.total_count))
        log("=" * 50)

        # å•çº¿ç¨‹ï¼šä¿æŒåŸæœ‰é¡ºåºä¸å»¶è¿Ÿé€»è¾‘ï¼ˆWeb é»˜è®¤è¡Œä¸ºï¼‰
        if max_concurrent <= 1:
            for idx, book_id in enumerate(book_ids):
                if self.is_cancelled:
                    log(t("dl_batch_cancelled"))
                    break

                self.current_index = idx + 1
                result = download_single_book(book_id, self.current_index)
                self.results.append(result)

                # å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                if idx < len(book_ids) - 1 and not self.is_cancelled:
                    time.sleep(delay_between_books)
        else:
            # å¹¶å‘ï¼šç”¨äº CLI / Actions ç­‰ç¯å¢ƒ
            results_by_index = {}
            results_lock = threading.Lock()

            with ThreadPoolExecutor(max_workers=max_concurrent) as executor:
                future_to_index = {
                    executor.submit(download_single_book, book_id, idx + 1): (idx + 1, book_id)
                    for idx, book_id in enumerate(book_ids)
                    if not self.is_cancelled
                }

                for future in as_completed(future_to_index):
                    index, raw_book_id = future_to_index[future]
                    try:
                        result = future.result()
                    except Exception as e:
                        result = {
                            'book_id': str(raw_book_id).strip(),
                            'book_name': f"ä¹¦ç±_{str(raw_book_id).strip()}",
                            'success': False,
                            'message': str(e),
                            'index': index,
                            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        }

                    with results_lock:
                        results_by_index[index] = result

            # æŒ‰è¾“å…¥é¡ºåºè¾“å‡ºç»“æœ
            for i in range(1, len(book_ids) + 1):
                if i in results_by_index:
                    self.results.append(results_by_index[i])
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in self.results if r['success'])
        failed_count = len(self.results) - success_count
        
        log("\n" + "=" * 50)
        log(t("dl_batch_summary"))
        log(t("dl_batch_stats_success", success_count))
        log(t("dl_batch_stats_fail", failed_count))
        log(t("dl_batch_stats_total", len(self.results)))
        
        if failed_count > 0:
            log("\n" + t("dl_batch_fail_list"))
            for r in self.results:
                if not r['success']:
                    log(f"   - ã€Š{r['book_name']}ã€‹: {r['message']}")
        
        return {
            'success': failed_count == 0,
            'message': t("dl_batch_complete", success_count, len(self.results)),
            'total': len(self.results),
            'success_count': success_count,
            'failed_count': failed_count,
            'results': self.results
        }


batch_downloader = BatchDownloader()


def signal_handler(sig, frame):
    """ä¿¡å·å¤„ç†"""
    print('\næ­£åœ¨å–æ¶ˆä¸‹è½½...')
    downloader_instance.cancel_download()
    batch_downloader.cancel()
    sys.exit(0)


if __name__ == "__main__":
    try:
        signal.signal(signal.SIGINT, signal_handler)
    except ValueError:
        pass
    
    print("ç•ªèŒ„å°è¯´ä¸‹è½½å™¨")
    print("="*50)
    print("1. å•æœ¬ä¸‹è½½")
    print("2. æ‰¹é‡ä¸‹è½½")
    mode = input("é€‰æ‹©æ¨¡å¼ (1/2, é»˜è®¤: 1): ").strip() or "1"
    
    save_path = input("è¯·è¾“å…¥ä¿å­˜è·¯å¾„(é»˜è®¤: ./novels): ").strip() or "./novels"
    file_format = input("é€‰æ‹©æ ¼å¼ (txt/epub, é»˜è®¤: txt): ").strip() or "txt"
    os.makedirs(save_path, exist_ok=True)
    
    if mode == "2":
        # æ‰¹é‡ä¸‹è½½æ¨¡å¼
        print("\nè¯·è¾“å…¥ä¹¦ç±IDåˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œè¾“å…¥ç©ºè¡Œç»“æŸï¼‰:")
        book_ids = []
        while True:
            line = input().strip()
            if not line:
                break
            # æ”¯æŒé€—å·/ç©ºæ ¼/æ¢è¡Œåˆ†éš”
            for bid in re.split(r'[,\s]+', line):
                bid = bid.strip()
                if bid:
                    book_ids.append(bid)
        
        if book_ids:
            print(f"\nå…± {len(book_ids)} æœ¬ä¹¦ç±å¾…ä¸‹è½½")
            result = batch_downloader.run_batch(book_ids, save_path, file_format)
            print(f"\næ‰¹é‡ä¸‹è½½ç»“æŸ: {result['message']}")
        else:
            print("æ²¡æœ‰è¾“å…¥ä¹¦ç±ID")
    else:
        # å•æœ¬ä¸‹è½½æ¨¡å¼
        book_id = input("è¯·è¾“å…¥ä¹¦ç±ID: ").strip()
        success = Run(book_id, save_path, file_format)
        if success:
            print("ä¸‹è½½å®Œæˆ!")
        else:
            print("ä¸‹è½½å¤±è´¥!")
